# Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks

### 这篇笔记的写作者是[AsTheStarsFall](https://github.com/asthestarsfalll)。

> 论文名称：[Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks](https://arxiv.org/pdf/2105.02358.pdf)
>
> 作者：Meng-Hao Guo, Zheng-Ning Liu, Tai-Jiang Mu, Shi-Min Hu, Senior Member
>
> Code：https://github.com/MenghaoGuo/EANet

## 前言

自从$Self-Attention$在NLP中被提出之后，许多工作尝试将其引入计算机视觉的各种任务之中，即使其被证明是一种行之有效的方法，其缺点终将会阻碍自身的进一步发展：

1. $Self-Attention$的计算复杂度呈二次增长
2. 只能学习到单个样本中的长距离关系，但是无法考虑所有样本之间的关联

因此，提出了$External-Attention$，仅仅通过两个可学习的单元，实现了线性的时间复杂度，并且能够考虑到所有样本之间的关联，与此同时，对整个网络起着重要的正则作用，有利于增强网络的泛化性

## External Attention



![image-20210614182722717](https://gitee.com/Thedeadleaf/images/raw/master/image-20210614182722717.png)

### Self Attention

公式为：
$$
F_{out}=softmax(W_QF(W_KF)^T)W_VF
$$
具体内容移步[此](https://asthestarsfalll.icu/2021/06/04/transformer/#self-attention)

另一个简化$Self\quad Attention$的公式为
$$
F_{out}=softmax(FF^T)F
$$
其时间复杂度为$O(dN^2)$

在计算机视觉的各类任务中，显然，我们不需要NXN的注意力图，因为像素点之间关系并不想NLP中词语之间的关系那么密切，因而有的工作提出了在patch上计算注意力。

即便能在一定程度上缓解计算消耗，$Self-Attention$忽略了不同样本间数据的关系，在一定程度上限制了其灵活度和泛化性

### External Attention

不再通过原始输入来获得$K,V$，而使用两个独立的$Memory\quad Unit$，公式如下：
$$
A=\alpha_{i,j}=Norm(FM^T_K)
$$
在原文中，这里的$\alpha_{i,j}$指的是第$i$个像素与$M_k$中第$j$行的相似度，我认为体现的并不明显（emmmm我太菜了）

让我们做一个简单的推理（这里的矩阵的长宽都假设为N）：
$$
FM_K^T=
\left[\begin{array}
{c}f_{1} & f_{2}&\cdots&f_{n}\\
f_{n+1}& f_{n+2}&\cdots&f_{2n}\\
\vdots&\vdots&\ddots&\vdots\\
f_{n^2-n+1}&f_{n^2-n+2}&\cdots&f_{n^2}
\end{array}\right]
\left[\begin{array}
{c}m_{1,1} & m_{2,1}&\cdots&m_{n,1}\\
m_{1,2}& m_{2,2}&\cdots&m_{n,2}\\
\vdots&\vdots&\ddots&\vdots\\
m_{1,n}&m_{2,n}&\cdots&m_{n,n}
\end{array}\right]
$$
根据矩阵的运算规则，对于$f_i$（它在第$j$列），那么$M_K^T$的第$j$行的所有元素都会和$f_i$相乘（注意这里是转置之后的行，在原来的$M_K$上应该是列，更加迷惑了），然而它们并不会加在一起，而是分布在一个维度$(1,n)$的矩阵中，并且还会有其他数据的影响，因为$f_i$所在的一行会与$M_K^T$的每一列乘加，

综上，我不知道相似度体现在那里

总之，ES将时间复杂度降到了线性，其输出为：
$$
F_{out}=AM_V
$$
多头External Attention的实现如下

![image-20210614203458185](https://gitee.com/Thedeadleaf/images/raw/master/image-20210614203458185.png)

### 归一化

我们知道，$Softmax$对输入的尺度大小十分敏感，因此在自注意力中使用了缩放点积来减小影响，具体移步[此](https://asthestarsfalll.icu/2021/06/04/transformer/#%E7%BC%A9%E6%94%BE%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B)

在ES中，使用$Double\quad Normalization$分别在行和列上进行归一化

这里的$Double\quad Normalization$实际上就是先对列进行$Softmax$再对行进行$l_1norm$

## 代码

```python
class External_attention(Module):
    '''
    Arguments:
        c (int): The input and output channel number.
    '''
    def __init__(self, c):
        super(External_attention, self).__init__()
        
        self.conv1 = nn.Conv2d(c, c, 1)

        self.k = 64
        self.linear_0 = nn.Conv1d(c, self.k, 1, bias=False)

        self.linear_1 = nn.Conv1d(self.k, c, 1, bias=False)
        self.linear_1.weight = self.linear_0.weight.permute(1, 0, 2) #初始化成一样是为了让 Mk 和 Mv 最开始时候是一致的，类似于 self-attention 里面的 K 和 V 都是从 X 经过线性变换来的，这种设置在训练初始阶段会稳定一些，但是貌似不会影响最终结果。        
        
        self.conv2 = nn.Sequential(
            nn.Conv2d(c, c, 1, bias=False),
            nn.BatchNorm(c))        

        self.relu = nn.ReLU()

    def execute(self, x):
        idn = x
        x = self.conv1(x)

        b, c, h, w = x.size()
        n = h*w
        x = x.view(b, c, h*w)   # b * c * n 

        attn = self.linear_0(x) # b, k, n
        attn = nn.softmax(attn, dim=-1) # b, k, n

        attn = attn / (1e-9 + attn.sum(dim=1, keepdims=True)) #  # b, k, n
        x = self.linear_1(attn) # b, c, n

        x = x.view(b, c, h, w)
        x = self.conv2(x)
        x = x + idn
        x = self.relu(x)
        return x

```

多头EA请自行前往仓库查找

## 思考

先看大佬怎么说：https://www.zhihu.com/search?type=content&q=External%20Attention

感觉External Attention与Bottleneck有一定的相似程度，首尾通过1X1卷积实现残差连接，不过在连接的过程中加入了Softmax和l1norm起到一定信息交换的作用

关于$Double\quad Normalization$：

如果你看完了输入尺度对$Softmax$的影响，应该会思考，为什么要将l1norm添加在后面呢？

当输入特征方差过大时，$Softmax$输出的会是一个接近于$One-hot$的向量，此时再做l1norm显然不会有什么变化吧

或者说这里的$Softmax$可以提取的是全局信息？

关于相似度：见上
