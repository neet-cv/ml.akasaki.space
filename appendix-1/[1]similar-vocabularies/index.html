<!doctype html>
<html lang="zh-cn" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-docs docs-doc-id-appendix-1/[1]similar-vocabularies">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">机器学习(?)术语表 | 工具箱的深度学习记事簿</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://ml.akasaki.space/appendix-1/[1]similar-vocabularies"><meta data-rh="true" name="docusaurus_locale" content="zh-cn"><meta data-rh="true" name="docsearch:language" content="zh-cn"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-docs-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-docs-current"><meta data-rh="true" property="og:title" content="机器学习(?)术语表 | 工具箱的深度学习记事簿"><meta data-rh="true" name="description" content="查找一个词条的办法是按下键盘上的Ctrl+F，然后键入关键字。"><meta data-rh="true" property="og:description" content="查找一个词条的办法是按下键盘上的Ctrl+F，然后键入关键字。"><link data-rh="true" rel="icon" href="/img/logo.svg"><link data-rh="true" rel="canonical" href="https://ml.akasaki.space/appendix-1/[1]similar-vocabularies"><link data-rh="true" rel="alternate" href="https://ml.akasaki.space/appendix-1/[1]similar-vocabularies" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://ml.akasaki.space/appendix-1/[1]similar-vocabularies" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="工具箱的深度学习记事簿 RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="工具箱的深度学习记事簿 Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.3231e09c.css">
<link rel="preload" href="/assets/js/runtime~main.416d7f00.js" as="script">
<link rel="preload" href="/assets/js/main.d42e6425.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">工具箱的深度学习记事簿</b></a><a class="navbar__item navbar__link" href="/blog">魔法部日志</a><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/about">Authors &amp; About</a><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">YetAnotherAkasaki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切换浅色/暗黑模式（当前为浅色模式）" aria-label="切换浅色/暗黑模式（当前为浅色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="搜索" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="回到顶部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="文档侧边栏" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/">首页</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/ch0/[1]ai-that-destroying-netizens">第零章：在开始之前</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/ch1p1/[1]operate-on-data">第一章上：HelloWorld</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/ch1p2/[1]multilayer-perceptron">第一章下：深度学习基础</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/ch2p1/[1]convolutional-nn-and-ops">第二章上：卷积神经网络</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/ch2p2/[1]LeNet">第二章下：经典卷积神经网络</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/ch3p1/[1]deep-learning-for-computer-vision">第三章上：谈一些计算机视觉方向</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/ch3p2/[1]bounding-box-and-anchor-box">第三章下：了解更高级的技术</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/appendix-1/[1]similar-vocabularies">附录1：好朋友们</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/appendix-1/[1]similar-vocabularies">机器学习(?)术语表</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/appendix-1/[2]activation-functions">激活函数们</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/appendix-1/[3]loss-functions">损失函数们</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/appendix-1/[4]similar-codeblocks">眼熟的代码块</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/appendix-1/[5]introducing-matplotlib">搬砖时应该掌握的matplotlib</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/appendix-1/[6]who-is-akasaki-toolbox">工具箱？</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/appendix-2/[1]bayesian-methods">附录2：数学是真正的圣经</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/appendix-3/[1]about-dsp">附录3：信号和采样的学问（DSP）</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/appendix-4/[1]operation-on-tensors-1">附录4：TensorFlow编程策略</a></div></li></ul></nav><button type="button" title="收起侧边栏" aria-label="收起侧边栏" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="页面路径"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主页面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">附录1：好朋友们</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">机器学习(?)术语表</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本页总览</button></div><div class="theme-doc-markdown markdown"><h1>机器学习(?)术语表</h1><p>查找一个词条的办法是按下键盘上的<code>Ctrl+F</code>，然后键入关键字。</p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="英文首字母索引">英文首字母索引：<a href="#英文首字母索引" class="hash-link" aria-label="英文首字母索引：的直接链接" title="英文首字母索引：的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-a">字母 A<a href="#字母-a" class="hash-link" aria-label="字母 A的直接链接" title="字母 A的直接链接">​</a></h3><ul><li><p>准确率（accuracy）</p><p>  <a href="https://developers.google.com/machine-learning/glossary/#classification_model" target="_blank" rel="noopener noreferrer"><strong>分类模型</strong></a>的正确预测所占的比例。在<a href="https://developers.google.com/machine-learning/glossary/#multi-class" target="_blank" rel="noopener noreferrer"><strong>多类别分类</strong></a>中，准确率的定义如下：</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>准确率</mtext><mo>=</mo><mfrac><mtext>正确的预测数</mtext><mtext>样本总数</mtext></mfrac></mrow><annotation encoding="application/x-tex">\text{准确率} = \frac{\text{正确的预测数}} {\text{样本总数}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord text"><span class="mord cjk_fallback">准确率</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.0463em;vertical-align:-0.686em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord text"><span class="mord cjk_fallback">样本总数</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord text"><span class="mord cjk_fallback">正确的预测数</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div></li></ul><p>​		在<a href="https://developers.google.com/machine-learning/glossary/#binary_classification" target="_blank" rel="noopener noreferrer"><strong>二元分类</strong></a>中，准确率的定义如下：</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>准确率</mtext><mo>=</mo><mfrac><mrow><mtext>正例数</mtext><mo>+</mo><mtext>负例数</mtext></mrow><mtext>样本总数</mtext></mfrac></mrow><annotation encoding="application/x-tex">\text{准确率} = \frac{\text{正例数} + \text{负例数}} {\text{样本总数}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord text"><span class="mord cjk_fallback">准确率</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.0463em;vertical-align:-0.686em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord text"><span class="mord cjk_fallback">样本总数</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord text"><span class="mord cjk_fallback">正例数</span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord text"><span class="mord cjk_fallback">负例数</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div><p>​		请参阅<a href="https://developers.google.com/machine-learning/glossary/#TP" target="_blank" rel="noopener noreferrer"><strong>正例</strong></a>和<a href="https://developers.google.com/machine-learning/glossary/#TN" target="_blank" rel="noopener noreferrer"><strong>负例</strong></a>。</p><ul><li><p>A/B testing （A/B 测试）</p><p>一种统计方法，用于将两种或多种技术进行比较，通常是将当前采用的技术与新技术进行比较。A/B 测试不仅旨在确定哪种技术的效果更好，而且还有助于了解相应差异是否具有显著的统计意义。A/B 测试通常是采用一种衡量方式对两种技术进行比较，但也适用于任意有限数量的技术和衡量方式。</p></li><li><p>激活函数 (activation function)</p><p>一种函数（常见的有ReLU、tanh等），用于对上一层的所有输入求加权和，然后生成一个输出值（通常为非线性值），并将其传递给下一层。</p></li><li><p>AdaGrad（Adaptive Subgradient Methods for Online Learning and Stochastic Optimization）</p><p>一种先进的梯度下降法，用于重新调整每个参数的梯度，以便有效地为每个参数指定独立的学习速率。如需查看完整的解释，请参阅<a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" target="_blank" rel="noopener noreferrer">这篇论文</a>。</p></li></ul><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-b">字母 B<a href="#字母-b" class="hash-link" aria-label="字母 B的直接链接" title="字母 B的直接链接">​</a></h3><ul><li><p>反向传播 (backpropagation)</p><p>在神经网络上执行梯度下降法的主要算法。该算法会先按前向传播方式计算（并缓存）每个节点的输出值，然后再按反向传播遍历图的方式计算损失函数值相对于每个参数的<a href="https://en.wikipedia.org/wiki/Partial_derivative" target="_blank" rel="noopener noreferrer">偏导数</a>。</p></li><li><p>批量标准化（BN，batch normalization）</p><p>BN 是由 Google 于 2015 年提出，这是一个深度神经网络训练的技巧，它不仅可以加快了模型的收敛速度，而且更重要的是在一定程度缓解了深层网络中“梯度弥散”的问题，从而使得训练深层网络模型更加容易和稳定。所以目前 BN 已经成为几乎所有卷积神经网络的标配技巧了。</p></li><li><p>基准 (baseline)</p><p>一种简单的模型或启发法，用作比较模型效果时的参考点。基准有助于模型开发者针对特定问题量化最低预期效果。</p></li><li><p>批次 (batch)</p><p>模型训练的一次迭代（即一次梯度更新）中使用的样本集。另请参阅<a href="https://developers.google.com/machine-learning/glossary/#batch_size" target="_blank" rel="noopener noreferrer"><strong>批次大小</strong></a>。</p></li><li><p>批次大小 (batch size)</p><p>一个批次中的样本数。例如，SGD的批次大小为 1，而小批次的大小通常介于 10 到 1000 之间。批次大小在训练和推断期间通常是固定的；不过，TensorFlow 允许使用动态批次大小。</p></li><li><p>偏差 (bias，偏差项)</p><p>距离原点的截距或偏移。偏差（也称为偏差项）在机器学习模型中用<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></span>或<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">w_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>表示。例如，在下面的公式中，偏差为 b：</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>y</mi><mtext>′</mtext><mo>=</mo><mi>b</mi><mo>+</mo><msub><mi>w</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mo>⋯</mo><msub><mi>w</mi><mi>n</mi></msub><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">y′ = b + w_1 x_1 + w_2 x_2 + \cdots w_n x_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.75em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mord">′</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span></div><p>请勿与预测偏差混淆。</p></li><li><p>二元分类 (binary classification)</p><p>一种分类任务，可输出两种互斥类别之一。例如，对电子邮件进行评估并输出“垃圾邮件”或“非垃圾邮件”的机器学习模型就是一个二元分类器。</p></li></ul><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-c">字母 C<a href="#字母-c" class="hash-link" aria-label="字母 C的直接链接" title="字母 C的直接链接">​</a></h3><ul><li><p>校准层 (calibration layer)</p><p>一种预测后调整，通常是为了降低预测偏差的影响。调整后的预测和概率应与观察到的标签集的分布一致。</p></li><li><p>分类模型（classification model）</p><p>一种能够进行分类任务的模型，可输出多种互斥类别之一。例如，对电子邮件进行评估并输出“垃圾邮件”或“非垃圾邮件”的机器学习模型就是一个二元分类器，而区分10种不同动物的模型就是一个十分类的分类器。</p></li><li><p>候选采样 (candidate sampling)</p><p>一种训练时进行的优化，会使用某种函数（例如 softmax）针对所有正类别标签计算概率，但对于负类别标签，则仅针对其随机样本计算概率。例如，如果某个样本的标签为“小猎犬”和“狗”，则候选采样将针对“小猎犬”和“狗”类别输出以及其他类别（猫、棒棒糖、栅栏）的随机子集计算预测概率和相应的损失项。这种采样基于的想法是，只要正类别始终得到适当的正增强，负类别就可以从频率较低的负增强中进行学习，这确实是在实际中观察到的情况。候选采样的目的是，通过不针对所有负类别计算预测结果来提高计算效率。</p></li><li><p>分类数据 (categorical data)</p><p>一种特征，拥有一组离散的可能值。以某个名为 <code>house style</code> 的分类特征为例，该特征拥有一组离散的可能值（共三个），即 <code>Tudor, ranch, colonial</code>。通过将 <code>house style</code> 表示成分类数据，相应模型可以学习 <code>Tudor</code>、<code>ranch</code> 和 <code>colonial</code> 分别对房价的影响。</p><p>有时，离散集中的值是互斥的，只能将其中一个值应用于指定样本。例如，<code>car maker</code> 分类特征可能只允许一个样本有一个值 (<code>Toyota</code>)。在其他情况下，则可以应用多个值。一辆车可能会被喷涂多种不同的颜色，因此，<code>car color</code> 分类特征可能会允许单个样本具有多个值（例如 <code>red</code> 和 <code>white</code>）。</p><p>分类特征有时称为离散特征。与数值数据相对。</p></li><li><p>形心 (centroid)</p><p>聚类的中心，由 k-means 或 k-median 算法的具体实现决定。例如，如果 k 为 3，则 k-means 或 k-median 算法会找出 3 个形心。</p></li><li><p>协变量（covariate）</p><p>在机器学习和深度学习方法中，协变量指的是与输入无关的其他变量。在计算机视觉中，输入一般指的是图像或特征图（feature map），而协变量可以指权重 w 等。举个例子：</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>S</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>K</mi><mi>T</mi><mi>t</mi><mo>+</mo><mi>e</mi></mrow><annotation encoding="application/x-tex">S(t) = KTt + e</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="mord mathnormal">Tt</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">e</span></span></span></span></span></div><p>其中， S(t)是降雨总量， t 是自变量时间，降雨量（t）是因变量，而温度（T）则是协变量，K 为一个常数。</p><p>通常情况下，在实验的设计中，协变量是一个独立变量（解释变量），不为实验者所操纵，但仍影响实验结果。</p></li><li><p>协变量偏移（covariate shift）</p></li><li><p>级联（cascade）</p><p>级联在计算机科学里指多个对象之间的映射关系，建立数据之间的级联关系提高管理效率。在计算机视觉中，级联通常用在由低层到高层的特征提取中。</p></li></ul><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-d">字母 D<a href="#字母-d" class="hash-link" aria-label="字母 D的直接链接" title="字母 D的直接链接">​</a></h3><ul><li><p>数据集（dataset）</p></li><li><p>解码器（decoder）</p><p>在计算机视觉中，解码器通常和编码器（encoder）同时出现，两者组合起来用于图像的分割任务。解码器通常由转置卷积层和上采样层组成，用于将编码器编码的特征“解码”回原图大小或接近原图的大小。</p></li><li><p>无量纲化（dimensionless，nondimensionalize）</p><p>当在某种运算中，两种不同量纲的变量对运算结果起着同等的作用，或它们对结果的重要性一样，它们往往可以当作同类的量处理，或同时进行<a href="//todo" target="_blank" rel="noopener noreferrer">归一化</a>处理</p></li></ul><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-e">字母 E<a href="#字母-e" class="hash-link" aria-label="字母 E的直接链接" title="字母 E的直接链接">​</a></h3><ul><li><p>编码器（encoder）</p><p>在计算机视觉中，编码器通常和解码器（decoder）组合起来用于图像的分割任务。编码器通常由CNN构成，用于将特征逐级提取，这个过程也可以被称为编码。</p></li></ul><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-f">字母 F<a href="#字母-f" class="hash-link" aria-label="字母 F的直接链接" title="字母 F的直接链接">​</a></h3><ul><li><p><strong>特征</strong>（feature）</p></li><li><p><strong>特征图</strong>（ feature map）</p></li><li><p><strong>向前传播</strong>（正向传播，forwarding，forward propagation）</p></li><li><p><strong>量纲</strong>（因次，fundamental unit）</p><p>物理学中，不同的物理量有着不同的单位，然而这些单位之间都有相互的联系。实际上，恰当地规定一些基本的单位（称为基本单位），可以使任何其他的单位（称为导出单位）都表达为这些单位的乘积，将其统一以便于研究各个物理量之间的关系。如在国际单位制中，功的单位焦耳（<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi></mrow><annotation encoding="application/x-tex">J</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.09618em">J</span></span></span></span></span>），可以表示为“千克平方米每平方秒”（<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mi>g</mi><mo>⋅</mo><msup><mi>m</mi><mn>2</mn></msup><mi mathvariant="normal">/</mi><msup><mi>s</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">kg \cdot m^2 / s^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord">/</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>）。</p><p>然而，仅仅用单位来表示会面临一些问题：</p><ol><li>在不同的单位制下，各个物理量用单位来表示也会不同，以至于起不到预期的“统一各单位”的效果。如英里每小时（<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>p</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">mph</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">p</span><span class="mord mathnormal">h</span></span></span></span></span>）与米每秒（<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi mathvariant="normal">/</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">m/s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">m</span><span class="mord">/</span><span class="mord mathnormal">s</span></span></span></span></span>）乍看之下无甚联系，然而它们却都是表示速度的单位。虽然说经过转换可以将各个基本单位也统一，然而这样终究不够直观，需记忆也不甚方便，而且选择哪一个单位作为统一单位似乎都不甚公平。</li><li>把一个既有的单位表达为拆分了的基本单位的形式实际上没有任何意义，功的单位无论如何都不是“千克二次方米每二次方秒”，因为实际上这个单位根本不存在，它只是与“焦耳”恰好相等而已。况且，这样做也会导致一些拆分后相同但实质不同的单位被混淆，如力矩的单位牛米（<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>⋅</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">N \cdot m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">m</span></span></span></span></span>）被拆分后也是（<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mi>g</mi><mo>⋅</mo><msup><mi>m</mi><mn>2</mn></msup><mi mathvariant="normal">/</mi><msup><mi>s</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">kg \cdot m^2 /s^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord">/</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>），然而它与功显然是完全不同的。</li></ol><p>因此量纲被作为表达导出单位组成的专有方式引入物理学中。</p></li><li><p><strong>特征缩放</strong>（feature scaling）</p><p>正在施工</p></li><li><p><strong>前馈神经网络</strong> ( feedforward neural network , FNN )</p><p>前馈神经网络（<a href="https://zh.wikipedia.org/wiki/%E8%8B%B1%E6%96%87" target="_blank" rel="noopener noreferrer">英文</a>：Feedforward Neural Network），为<a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD" target="_blank" rel="noopener noreferrer">人工智能</a>領域中，最早发明的简单<a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" target="_blank" rel="noopener noreferrer">人工神经网络</a>类型。在它内部，参数从输入层向输出层单向传播。有异于<a href="https://zh.wikipedia.org/wiki/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" target="_blank" rel="noopener noreferrer">循环神经网络</a>和<a href="https://zh.wikipedia.org/wiki/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" target="_blank" rel="noopener noreferrer">递归神经网络</a>，它的内部不会构成<a href="https://zh.wikipedia.org/wiki/%E7%92%B0_(%E5%9C%96%E8%AB%96)" target="_blank" rel="noopener noreferrer">有向环</a>。FNN由一个输入层、一个（浅层网络）或多个（深层网络，因此叫作深度学习）隐藏层，和一个输出层构成。每个层（除输出层以外）与下一层连接。这种连接是 FNN 架构的关键，具有两个主要特征：加权平均值和激活函数。</p><p>一般情况下，前馈神经网络被分为<a href="https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E6%9C%BA" target="_blank" rel="noopener noreferrer">单层感知机</a>和<a href="https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA" target="_blank" rel="noopener noreferrer">多层感知机</a>。</p></li></ul><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-g">字母 G<a href="#字母-g" class="hash-link" aria-label="字母 G的直接链接" title="字母 G的直接链接">​</a></h3><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-h">字母 H<a href="#字母-h" class="hash-link" aria-label="字母 H的直接链接" title="字母 H的直接链接">​</a></h3><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-i">字母 I<a href="#字母-i" class="hash-link" aria-label="字母 I的直接链接" title="字母 I的直接链接">​</a></h3><ul><li>输入（输入数据，input，input data，input dataset）</li></ul><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-j">字母 J<a href="#字母-j" class="hash-link" aria-label="字母 J的直接链接" title="字母 J的直接链接">​</a></h3><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-k">字母 K<a href="#字母-k" class="hash-link" aria-label="字母 K的直接链接" title="字母 K的直接链接">​</a></h3><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-l">字母 L<a href="#字母-l" class="hash-link" aria-label="字母 L的直接链接" title="字母 L的直接链接">​</a></h3><ul><li><p>标签（label）</p></li><li><p>损失函数（loss function）</p></li></ul><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-m">字母 M<a href="#字母-m" class="hash-link" aria-label="字母 M的直接链接" title="字母 M的直接链接">​</a></h3><ul><li><p><strong>多层感知机</strong>（人工神经网络，MLP，multilayer perceptron，ANN，artificial neural network）</p><p>多层感知器（Multilayer Perceptron,缩写MLP）是一种<a href="https://zh.wikipedia.org/wiki/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" target="_blank" rel="noopener noreferrer">前向结构</a>的<a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" target="_blank" rel="noopener noreferrer">人工神经网络</a>，映射一组输入向量到一组输出向量。MLP可以被看作是一个有向图，由多个的节点层所组成，每一层都全连接到下一层。除了输入节点，每个节点都是一个带有非线性激活函数的神经元（或称处理单元）。一种被称为<a href="https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95" target="_blank" rel="noopener noreferrer">反向传播算法</a>的<a href="https://zh.wikipedia.org/wiki/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0" target="_blank" rel="noopener noreferrer">监督学习</a>方法常被用来训练MLP。多层感知器遵循人类神经系统原理，学习并进行数据预测。它首先学习，然后使用权重存储数据，并使用算法来调整权重并减少训练过程中的偏差，即实际值和预测值之间的误差。主要优势在于其快速解决复杂问题的能力。多层感知的基本结构由三层组成：第一输入层，中间隐藏层和最后输出层，输入元素和权重的乘积被馈给具有神经元偏差的求和结点,主要优势在于其快速解决复杂问题的能力。MLP是<a href="https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8" target="_blank" rel="noopener noreferrer">感知器</a>的推广，克服了感知器不能对<a href="https://zh.wikipedia.org/w/index.php?title=%E7%BA%BF%E6%80%A7%E4%B8%8D%E5%8F%AF%E5%88%86&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener noreferrer">线性不可分</a>数据进行识别的弱点。</p></li></ul><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-n">字母 N<a href="#字母-n" class="hash-link" aria-label="字母 N的直接链接" title="字母 N的直接链接">​</a></h3><ul><li><p>样本归一化（归一化，normalization）</p><p>通常来说，样本标准化是指特征工程中的<a href="//todo" target="_blank" rel="noopener noreferrer">特征缩放</a>过程。</p><p>在数值上，归一化一般将把数据变成<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(0,1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span>或<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(-1,1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span>之间的小数。</p><p>归一化/标准化实质是一种线性变换，线性变换有很多良好的性质，这些性质决定了对数据改变后不会造成“失效”，反而能提高数据的表现，这些性质是归一化/标准化的前提。比如有一个很重要的性质：线性变换不会改变原始数据的数值排序。</p><p>在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，即提升模型的收敛速度。</p><p>简单的线性数值归一化运算常见的表示形式有：</p><ol><li>Min-Max Normalization： <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mo stretchy="false">(</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x&#x27; = (x - min(x)) / (max(x) - min(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">min</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span><span class="mord">/</span><span class="mopen">(</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">min</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span></span></li><li>平均归一化：<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mi>μ</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mo stretchy="false">(</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x&#x27; = (x - μ) / (max(x) - min(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">μ</span><span class="mclose">)</span><span class="mord">/</span><span class="mopen">(</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">min</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span></span></li></ol><p>简单的线性标准化可能存在的缺陷是当有新数据加入时，可能导致<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">max(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>和<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">min(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">min</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>的变化，需要重新定义。</p><p>非线性的归一化可能的表示形式有：</p><ol><li>对数函数转换：<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x&#x27; = \log(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></li><li>反余切函数转换：<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mi>arctan</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>⋅</mo><mn>2</mn><mi mathvariant="normal">/</mi><mi>π</mi></mrow><annotation encoding="application/x-tex">x&#x27; = \arctan(x) \cdot 2 / π</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop">arctan</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">2/</span><span class="mord mathnormal" style="margin-right:0.03588em">π</span></span></span></span></span></li></ol><p>非线性的标准化方法经常用在数据分化比较大的场景，即有些数值很大，有些很小。通过一些数学函数（包括 log、指数，正切等），将原始值进行映射。根据数据分布的情况，可能会决定使用不同的非线性函数的曲线。并没有完全通用的标准化方法。</p></li><li><p>无量纲化（nondimensionalize，dimensionless）</p><p>当在某种运算中，两种不同量纲的变量对运算结果起着同等的作用，或它们对结果的重要性一样，它们往往可以当作同类的量处理，或同时进行<a href="//todo" target="_blank" rel="noopener noreferrer">归一化</a>处理</p></li></ul><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-o">字母 O<a href="#字母-o" class="hash-link" aria-label="字母 O的直接链接" title="字母 O的直接链接">​</a></h3><ul><li>输出（输出数据，output，output dataset）</li><li>过拟合（overfitting）</li><li>one-hot编码（onehot，one hot encoding）</li><li></li></ul><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-p">字母 P<a href="#字母-p" class="hash-link" aria-label="字母 P的直接链接" title="字母 P的直接链接">​</a></h3><ul><li><strong>感知机</strong></li></ul><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-q">字母 Q<a href="#字母-q" class="hash-link" aria-label="字母 Q的直接链接" title="字母 Q的直接链接">​</a></h3><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-r">字母 R<a href="#字母-r" class="hash-link" aria-label="字母 R的直接链接" title="字母 R的直接链接">​</a></h3><ul><li>随机梯度下降（random / stochastic gradient descent）</li></ul><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-s">字母 S<a href="#字母-s" class="hash-link" aria-label="字母 S的直接链接" title="字母 S的直接链接">​</a></h3><ul><li><p>样本（sample）</p></li><li><p>标准化（样本标准化，standardization）</p><p>在机器学习和深度学习中，我们可能要处理不同种类的资料，例如，音讯和图片上的像素值，这些资料可能是高维度的，资料标准化后会使每个特征中的数值平均变为 0(将每个特征的值都减掉原始资料中该特征的平均)、标准差变为 1，这个方法被广泛的使用在许多机器学习算法中(例如：支持向量机、逻辑回归和类神经网络)。</p></li><li><p>随机梯度下降（random / stochastic gradient descent）</p></li><li><p>有监督学习（监督学习，监督式学习，supervised learning）</p><p>一句话讲完：通过已有的一部分输入数据与输出数据之间的对应关系，生成一个函数，将输入映射到合适的输出，例如分类。</p><p>多说一点：监督式学习（Supervised learning），是一个机器学习中的方法，可以由训练资料中学到或建立一个模式（ learning model），并依此模式推测新的实例。训练资料是由输入物件（通常是向量）和预期输出所组成。函数的输出可以是一个连续的值（称为回归分析），或是预测一个分类标签（称作分类）。</p></li></ul><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-t">字母 T<a href="#字母-t" class="hash-link" aria-label="字母 T的直接链接" title="字母 T的直接链接">​</a></h3><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-u">字母 U<a href="#字母-u" class="hash-link" aria-label="字母 U的直接链接" title="字母 U的直接链接">​</a></h3><ul><li><p>欠拟合（underfitting）</p></li><li><p>半监督学习（unsupervised learning）</p><p>综合利用有类标的数据和没有类标的数据，来生成合适的分类函数。</p></li><li><p>无监督学习（非监督学习，unsupervised learning）</p><p>直接对输入数据集进行建模，例如聚类。</p></li></ul><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-v">字母 V<a href="#字母-v" class="hash-link" aria-label="字母 V的直接链接" title="字母 V的直接链接">​</a></h3><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-w">字母 W<a href="#字母-w" class="hash-link" aria-label="字母 W的直接链接" title="字母 W的直接链接">​</a></h3><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-x">字母 X<a href="#字母-x" class="hash-link" aria-label="字母 X的直接链接" title="字母 X的直接链接">​</a></h3><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-y">字母 Y<a href="#字母-y" class="hash-link" aria-label="字母 Y的直接链接" title="字母 Y的直接链接">​</a></h3><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="字母-z">字母 Z<a href="#字母-z" class="hash-link" aria-label="字母 Z的直接链接" title="字母 Z的直接链接">​</a></h3><ul><li><p>中心化（零均值化，zero centered）</p><p>中心化后要求样本平均值为 0，对标准差无要求</p></li></ul><hr><h1>Other tips</h1><ul><li>标准化、归一化、中心化的区别</li></ul><ol><li>标准化和中心化的区别：标准化是原始分数减去平均数然后除以标准差，中心化是原始分数减去平均数。 所以一般流程为先中心化再标准化。</li><li>归一化和标准化的区别：归一化是将样本的特征值转换到同一量纲下把数据映射到<!-- -->[0,1]<!-- -->或者<!-- -->[-1, 1]<!-- -->区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。标准化是依照特征矩阵的列处理数据，其通过求 z-score 的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量 X 按照比例压缩再进行平移。</li></ol><hr><h1>前方正在施工</h1><ul><li>上采样（up sampling）</li><li>下采样（降采样，down sampling,sub sampling）</li><li>梯度弥散</li><li>梯度爆炸</li><li>梯度消失</li><li>特征缩放</li><li>循环神经网络（RNN，recursive neural network）</li><li>长短期记忆（LSTM，long short term memory）</li><li>前馈神经网络（feedforward neural network）</li></ul></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文档分页导航"><a class="pagination-nav__link pagination-nav__link--prev" href="/ch3p2/[7]vision-transformer"><div class="pagination-nav__sublabel">上一页</div><div class="pagination-nav__label">Transformer到Vision Transformer</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/appendix-1/[2]activation-functions"><div class="pagination-nav__sublabel">下一页</div><div class="pagination-nav__label">激活函数们</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#英文首字母索引" class="table-of-contents__link toc-highlight">英文首字母索引：</a><ul><li><a href="#字母-a" class="table-of-contents__link toc-highlight">字母 A</a></li><li><a href="#字母-b" class="table-of-contents__link toc-highlight">字母 B</a></li><li><a href="#字母-c" class="table-of-contents__link toc-highlight">字母 C</a></li><li><a href="#字母-d" class="table-of-contents__link toc-highlight">字母 D</a></li><li><a href="#字母-e" class="table-of-contents__link toc-highlight">字母 E</a></li><li><a href="#字母-f" class="table-of-contents__link toc-highlight">字母 F</a></li><li><a href="#字母-g" class="table-of-contents__link toc-highlight">字母 G</a></li><li><a href="#字母-h" class="table-of-contents__link toc-highlight">字母 H</a></li><li><a href="#字母-i" class="table-of-contents__link toc-highlight">字母 I</a></li><li><a href="#字母-j" class="table-of-contents__link toc-highlight">字母 J</a></li><li><a href="#字母-k" class="table-of-contents__link toc-highlight">字母 K</a></li><li><a href="#字母-l" class="table-of-contents__link toc-highlight">字母 L</a></li><li><a href="#字母-m" class="table-of-contents__link toc-highlight">字母 M</a></li><li><a href="#字母-n" class="table-of-contents__link toc-highlight">字母 N</a></li><li><a href="#字母-o" class="table-of-contents__link toc-highlight">字母 O</a></li><li><a href="#字母-p" class="table-of-contents__link toc-highlight">字母 P</a></li><li><a href="#字母-q" class="table-of-contents__link toc-highlight">字母 Q</a></li><li><a href="#字母-r" class="table-of-contents__link toc-highlight">字母 R</a></li><li><a href="#字母-s" class="table-of-contents__link toc-highlight">字母 S</a></li><li><a href="#字母-t" class="table-of-contents__link toc-highlight">字母 T</a></li><li><a href="#字母-u" class="table-of-contents__link toc-highlight">字母 U</a></li><li><a href="#字母-v" class="table-of-contents__link toc-highlight">字母 V</a></li><li><a href="#字母-w" class="table-of-contents__link toc-highlight">字母 W</a></li><li><a href="#字母-x" class="table-of-contents__link toc-highlight">字母 X</a></li><li><a href="#字母-y" class="table-of-contents__link toc-highlight">字母 Y</a></li><li><a href="#字母-z" class="table-of-contents__link toc-highlight">字母 Z</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">鲁ICP备2021025239号-2 Copyright © 2023 neet-cv.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.416d7f00.js"></script>
<script src="/assets/js/main.d42e6425.js"></script>
</body>
</html>