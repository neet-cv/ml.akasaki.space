"use strict";(self.webpackChunkml_notebook=self.webpackChunkml_notebook||[]).push([[758],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>b});var r=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=r.createContext({}),c=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},m=function(e){var t=c(e.components);return r.createElement(s.Provider,{value:t},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},f=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,a=e.originalType,s=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),p=c(n),f=o,b=p["".concat(s,".").concat(f)]||p[f]||u[f]||a;return n?r.createElement(b,i(i({ref:t},m),{},{components:n})):r.createElement(b,i({ref:t},m))}));function b(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=n.length,i=new Array(a);i[0]=f;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[p]="string"==typeof e?e:o,i[1]=l;for(var c=2;c<a;c++)i[c]=n[c];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}f.displayName="MDXCreateElement"},60039:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>y,contentTitle:()=>g,default:()=>v,frontMatter:()=>b,metadata:()=>d,toc:()=>h});var r=n(3905),o=Object.defineProperty,a=Object.defineProperties,i=Object.getOwnPropertyDescriptors,l=Object.getOwnPropertySymbols,s=Object.prototype.hasOwnProperty,c=Object.prototype.propertyIsEnumerable,m=(e,t,n)=>t in e?o(e,t,{enumerable:!0,configurable:!0,writable:!0,value:n}):e[t]=n,p=(e,t)=>{for(var n in t||(t={}))s.call(t,n)&&m(e,n,t[n]);if(l)for(var n of l(t))c.call(t,n)&&m(e,n,t[n]);return e},u=(e,t)=>a(e,i(t)),f=(e,t)=>{var n={};for(var r in e)s.call(e,r)&&t.indexOf(r)<0&&(n[r]=e[r]);if(null!=e&&l)for(var r of l(e))t.indexOf(r)<0&&c.call(e,r)&&(n[r]=e[r]);return n};const b={title:"Fast-SCNN - Fast Semantic Segmentation Network",authors:["zerorains"],tags:["segmentation","backbone"]},g=void 0,d={permalink:"/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network",editUrl:"https://github.dev/neet-cv/ml.akasaki.space/blob/master/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network.md",source:"@site/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network.md",title:"Fast-SCNN - Fast Semantic Segmentation Network",description:"\u8fd9\u662f\u4e00\u7bc7\u8bb2\u89e3\u4e00\u79cd\u5feb\u901f\u8bed\u4e49\u5206\u5272\u7684\u8bba\u6587\u3002\u8bba\u6587\u540d Fast Semantic Segmentation Network",date:"2023-12-31T09:31:53.000Z",formattedDate:"2023\u5e7412\u670831\u65e5",tags:[{label:"segmentation",permalink:"/blog/tags/segmentation"},{label:"backbone",permalink:"/blog/tags/backbone"}],readingTime:14.145,hasTruncateMarker:!0,authors:[{name:"Zerorains",title:"life is but a span, I use python",url:"https://github.com/zeroRains",email:"me@zerorains.top",imageURL:"https://github.com/zeroRains.png",key:"zerorains"}],frontMatter:{title:"Fast-SCNN - Fast Semantic Segmentation Network",authors:["zerorains"],tags:["segmentation","backbone"]},prevItem:{title:"MobileNetV2 - Inverted Residuals and Linear Bottlenecks",permalink:"/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck"},nextItem:{title:"MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications",permalink:"/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications"}},y={authorsImageUrls:[void 0]},h=[],w={toc:h},k="wrapper";function v(e){var t=e,{components:n}=t,o=f(t,["components"]);return(0,r.kt)(k,u(p(p({},w),o),{components:n,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"\u8fd9\u662f\u4e00\u7bc7\u8bb2\u89e3\u4e00\u79cd\u5feb\u901f\u8bed\u4e49\u5206\u5272\u7684\u8bba\u6587\u3002\u8bba\u6587\u540d:",(0,r.kt)("a",p({parentName:"p"},{href:"https://arxiv.org/abs/1902.04502"}),"Fast-SCNN: Fast Semantic Segmentation Network")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"\u4e3b\u8981\u662f\u91c7\u7528\u53cc\u6d41\u6a21\u578b\u7684\u67b6\u6784\u8bbe\u8ba1\u8fd9\u4e2a\u7f51\u7edc"),(0,r.kt)("li",{parentName:"ul"},"\u672c\u6587\u603b\u601d\u8def\uff1a\u51cf\u5c11\u5197\u4f59\u7684\u5377\u79ef\u8fc7\u7a0b\uff0c\u4ece\u800c\u63d0\u9ad8\u901f\u5ea6")),(0,r.kt)("p",null,"\u6458\u8981\uff1a"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"The encoder-decoder framework is state-of-the-art for offline semantic image segmentation. Since the rise in autonomous systems, real-time computation is increasingly desirable. In this paper, we introduce fast segmentation convolutional neural network (Fast-SCNN), an above real-time semantic segmentation model on high resolution image data (1024 \xd7 2048px) suited to efficient computation on embedded devices with low memory. Building on existing two-branch methods for fast segmentation, we introduce our \u2018learning to downsample\u2019 module which computes low-level features for multiple resolution branches simultaneously. Our network combines spatial detail at high resolution with deep features extracted at lower resolution, yielding an accuracy of 68.0% mean intersection over union at 123.5 frames per second on Cityscapes. We also show that large scale pre-training is unnecessary. We thoroughly validate our metric in experiments with ImageNet pre-training and the coarse labeled data of Cityscapes. Finally, we show even faster computation with competitive results on subsampled inputs, without any network modifications.")))}v.isMDXComponent=!0}}]);