"use strict";(self.webpackChunkml_notebook=self.webpackChunkml_notebook||[]).push([[1873],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>d});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),c=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=c(e.components);return r.createElement(l.Provider,{value:t},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},g=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),m=c(n),g=a,d=m["".concat(l,".").concat(g)]||m[g]||u[g]||o;return n?r.createElement(d,i(i({ref:t},p),{},{components:n})):r.createElement(d,i({ref:t},p))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=g;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[m]="string"==typeof e?e:a,i[1]=s;for(var c=2;c<o;c++)i[c]=n[c];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}g.displayName="MDXCreateElement"},14913:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>h,contentTitle:()=>f,default:()=>k,frontMatter:()=>d,metadata:()=>b,toc:()=>v});var r=n(3905),a=Object.defineProperty,o=Object.defineProperties,i=Object.getOwnPropertyDescriptors,s=Object.getOwnPropertySymbols,l=Object.prototype.hasOwnProperty,c=Object.prototype.propertyIsEnumerable,p=(e,t,n)=>t in e?a(e,t,{enumerable:!0,configurable:!0,writable:!0,value:n}):e[t]=n,m=(e,t)=>{for(var n in t||(t={}))l.call(t,n)&&p(e,n,t[n]);if(s)for(var n of s(t))c.call(t,n)&&p(e,n,t[n]);return e},u=(e,t)=>o(e,i(t)),g=(e,t)=>{var n={};for(var r in e)l.call(e,r)&&t.indexOf(r)<0&&(n[r]=e[r]);if(null!=e&&s)for(var r of s(e))t.indexOf(r)<0&&c.call(e,r)&&(n[r]=e[r]);return n};const d={title:"PointRend - Image Segmentation as Rendering",authors:["visualdust"],tags:["segmentation","refinement"]},f=void 0,b={permalink:"/blog/[19]PointRend-Image-Segmentation-as-Rendering",editUrl:"https://github.dev/neet-cv/ml.akasaki.space/blob/master/blog/[19]PointRend-Image-Segmentation-as-Rendering.md",source:"@site/blog/[19]PointRend-Image-Segmentation-as-Rendering.md",title:"PointRend - Image Segmentation as Rendering",description:"image-20210601121147760",date:"2023-12-31T09:31:53.000Z",formattedDate:"2023\u5e7412\u670831\u65e5",tags:[{label:"segmentation",permalink:"/blog/tags/segmentation"},{label:"refinement",permalink:"/blog/tags/refinement"}],readingTime:17.905,hasTruncateMarker:!0,authors:[{name:"Gavin Gong",title:"Rubbish CVer | Poor LaTex speaker | Half stack developer | \u952e\u5708\u8eba\u5c38\u7816\u5bb6",url:"https://gong.host",email:"gavin@gong.host",imageURL:"https://github.yuuza.net/visualDust.png",key:"visualdust"}],frontMatter:{title:"PointRend - Image Segmentation as Rendering",authors:["visualdust"],tags:["segmentation","refinement"]},prevItem:{title:"Involution - Inverting the Inherence of Convolution for Visual Recognition",permalink:"/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition"},nextItem:{title:"Transformer - Attention is all you need",permalink:"/blog/[20]Transformer-Attention-is-all-you-need"}},h={authorsImageUrls:[void 0]},v=[{value:"Abstract\uff08\u6458\u8981\uff09",id:"abstract\u6458\u8981",level:2}],y={toc:v},O="wrapper";function k(e){var t=e,{components:a}=t,o=g(t,["components"]);return(0,r.kt)(O,u(m(m({},y),o),{components:a,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"image-20210601121147760",src:n(75803).Z,width:"852",height:"454"})),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"\u201c\u6211\u4eec\u5e0c\u671b\u9884\u6d4b\u5206\u5272\u56fe\u7684\u8fb9\u754c\u533a\u57df\u66f4\u52a0\u51c6\u786e\uff0c\u6211\u4eec\u5c31\u4e0d\u5e94\u8be5\u4f7f\u7528\u5747\u5300\u91c7\u6837\uff0c\u800c\u5e94\u8be5\u66f4\u52a0\u503e\u5411\u4e8e\u56fe\u50cf\u8fb9\u754c\u533a\u57df\u3002\u201d")),(0,r.kt)("p",null,"\u8fd9\u662f\u4e00\u7bc7\u7528\u4e8e\u6539\u5584\u56fe\u50cf\u5206\u5272\u95ee\u9898\u4e2d\u8fb9\u7f18\u5206\u5272\u6548\u679c\u7684\u65b9\u6cd5\u7684\u8bba\u6587\u7684\u9605\u8bfb\u7b14\u8bb0\u3002\u8be5\u65b9\u6cd5\u201c\u5c06\u5206\u5272\u95ee\u9898\u770b\u4f5c\u6e32\u67d3\u95ee\u9898\u201d\uff0c\u8fbe\u5230\u4e86\u8f83\u597d\u7684\u6548\u679c\u3002\u8bba\u6587\u539f\u6587\uff1a",(0,r.kt)("a",m({parentName:"p"},{href:"https://arxiv.org/abs/1912.08193"}),"PointRend: Image Segmentation as Rendering"),"\u3002\u5728\u9605\u8bfb\u8fd9\u7bc7\u7b14\u8bb0\u4e4b\u524d\uff0c\u8bf7\u786e\u4fdd\u5148\u4e86\u89e3\u56fe\u50cf\u5206\u5272\u6280\u672f\u3002\u5bf9\u5206\u5272\u7684\u6280\u672f\u8fdb\u884c\u7b80\u8981\u7684\u4e86\u89e3\uff0c\u53ef\u4ee5\u53c2\u8003",(0,r.kt)("a",m({parentName:"p"},{href:"./%5B10%5DOverview-Of-Semantic-Segmentation"}),"\u53e6\u4e00\u7bc7\u7b14\u8bb0"),"\u3002"),(0,r.kt)("h2",m({},{id:"abstract\u6458\u8981"}),"Abstract\uff08\u6458\u8981\uff09"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over- and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend's efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at ",(0,r.kt)("a",m({parentName:"p"},{href:"https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend"}),"this https URL"),".")))}k.isMDXComponent=!0},75803:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/image-20210601121147760-2609d0f40692dce369b8041f749ff63b.png"}}]);