<!doctype html>
<html lang="zh-cn" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">The Devil is in the Decoder - Classification, Regression and GANs | 工具箱的深度学习记事簿</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://ml.akasaki.space/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs"><meta data-rh="true" name="docusaurus_locale" content="zh-cn"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="zh-cn"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="The Devil is in the Decoder - Classification, Regression and GANs | 工具箱的深度学习记事簿"><meta data-rh="true" name="description" content="这是一篇讲各种各样解码器的论文。原论文（The Devil is in the Decoder: Classification, Regression and GANs）。"><meta data-rh="true" property="og:description" content="这是一篇讲各种各样解码器的论文。原论文（The Devil is in the Decoder: Classification, Regression and GANs）。"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2023-12-31T09:31:53.000Z"><meta data-rh="true" property="article:author" content="https://gong.host"><meta data-rh="true" property="article:tag" content="survey,decoder,segmentation"><link data-rh="true" rel="icon" href="/img/logo.svg"><link data-rh="true" rel="canonical" href="https://ml.akasaki.space/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs"><link data-rh="true" rel="alternate" href="https://ml.akasaki.space/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://ml.akasaki.space/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="工具箱的深度学习记事簿 RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="工具箱的深度学习记事簿 Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.3231e09c.css">
<link rel="preload" href="/assets/js/runtime~main.416d7f00.js" as="script">
<link rel="preload" href="/assets/js/main.d42e6425.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">工具箱的深度学习记事簿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">魔法部日志</a><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/about">Authors &amp; About</a><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">YetAnotherAkasaki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切换浅色/暗黑模式（当前为浅色模式）" aria-label="切换浅色/暗黑模式（当前为浅色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="搜索" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="最近博文导航"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[00]unlimited-paper-works">欢迎来到魔法部日志</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[06]DeepLab-Series">DeepLab Series</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks - A Survey</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution - Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering">PointRend - Image Segmentation as Rendering</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[20]Transformer-Attention-is-all-you-need">Transformer - Attention is all you need</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features">RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet - Low-Light Enhancement Network with Global Awareness</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[26]CBAM-Convolutional-Block-Attention-Module">CBAM - Convolutional Block Attention Module</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO - Vision Outlooker for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention - Towards High-quality Pixel-wise Regression</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">CCNet - Criss-Cross Attention for Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">RepVGG - Making VGG-style ConvNets Great Again</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">PP-LCNet - A Lightweight CPU Convolutional Neural Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">Swin Transformer - Hierarchical Vision Transformer using Shifted Windows</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[46]Demystifying-Local-Vision-Transformer">Demystifying Local Vision Transformer</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation">DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations">GhostNet - More Features from Cheap Operations</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer">Kindling the Darkness - A Practical Low-light Image Enhancer</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[51]How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode">How much Position Information Do Convolutional Neural Networks Encode?</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[52]Axiomatic-Attribution-for-Deep-Networks">Axiomatic Attribution for Deep Networks</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="title_f1Hy" itemprop="headline">The Devil is in the Decoder - Classification, Regression and GANs</h1><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 15 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | 键圈躺尸砖家</small></div></div></div></div></header><div id="post-content" class="markdown" itemprop="articleBody"><p>这是一篇讲各种各样解码器的论文。<a href="https://arxiv.org/pdf/1707.05847.pdf" target="_blank" rel="noopener noreferrer">原论文（The Devil is in the Decoder: Classification, Regression and GANs）</a>。</p><p>由于“解码器（decoder，有些时候也被称为feature extractor）”的概念与像素级的分类、回归等问题多多少少都有瓜葛。以下是decoder被应用于像素级的任务：</p><ul><li>分类：语义分割、边缘检测。</li><li>回归：人体关键点检测、深度预测、着色、超分辨。</li><li>合成：利用生成对抗网络生成图像等。</li></ul><p>所以decoder是稠密预测（Dence prediction，像素级别的很多问题都可以叫做稠密的）问题的关键。</p><h1>Abstract（摘要）</h1><blockquote><p>Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.</p></blockquote><p>我看了这篇综述受益匪浅，如果有时间的话请阅读<a href="/papers/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.pdf">原作</a>。本文只是对原作阅读的粗浅笔记。</p><hr><p>​		语义分割、深度预测等计算机视觉任务往往需要对输入进行逐像素的预测，用于解决此类问题的模块通常由编码器组成。编码器（行为上是下采样的，通常情况下是卷积、池化组成的）在学习高维度特征的同时会降低输入图像的空间分辨率；在这之后是将其恢复原始分辨率的解码器（行为是上采样的，通常情况下是转置卷积等操作组成的）：</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">编码器（特征提取器，降低特征图分辨率）---解码器（提高特征图分辨率）</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="相关研究related-works">相关研究（Related works）<a href="#相关研究related-works" class="hash-link" aria-label="相关研究（Related works）的直接链接" title="相关研究（Related works）的直接链接">​</a></h2><p>​		这篇论文主要的内容是针对各种像素级的计算机视觉任务，对各种解码器进行了较为广泛的比较。以下是这篇论文的主要贡献：</p><ol><li>提出选择不同类型的解码器对效果的影响非常巨大</li><li>为解码器引入了类似残差（residual connection）的新连接</li><li>介绍了一种比较新颖的解码器：双线性加和上采样（bilinear additive upsampaling）</li><li>prediction artifacts（真的没想好怎么翻译）</li></ol><hr><p><img loading="lazy" alt="image-20210502073352860" src="/assets/images/image-20210502073352860-8730c4bffd2a6adf8d36499935c20517.png" width="654" height="290" class="img_ev3q"></p><p>我们将需要逐像素预测的问题成为密集预测（dence prediction）的问题。通常编码器-解码器结构是用于解决这种密集预测问题的：首先，编码器（特征提取器）在增加通道数量的同时降低了图像的空间分辨率（通常为8~32倍）；然后，解码器进行上采样恢复到输入原图大小。从概念上讲，此类解码器可以被视为和编码器相反的操作：一个解码器至少由一个提高空间分辨率的层（通常称为上采样层）以及可能保持空间分辨率的层（例如单位卷积、残差快或是起始块）组成。其中 ，用于保持空间分辨率的层已经有了比较成熟的研究，所以这一篇论文只分析提升空间分辨率的部分。</p><p>目前在单个计算机视觉领域内使用最多的是转置卷积（transposed convolution），它在分割、深度预测、超分辨重建等任务中都有比较详细的论文进行研究。详见原论文中的相关字段。</p><p>还有一些为了加快模型速度进行的研究，例如：二维卷积在图像分类和语义分割的背景下被分解成两个一维卷；还有一些比较新颖的堆叠的沙漏结构（似乎也可以叫金字塔结构），它是由堆叠的多个编码器-解码器组成。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="现存的上采样层设计existing-upsampling-layers">现存的上采样层设计Existing upsampling layers）<a href="#现存的上采样层设计existing-upsampling-layers" class="hash-link" aria-label="现存的上采样层设计Existing upsampling layers）的直接链接" title="现存的上采样层设计Existing upsampling layers）的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="转置卷积transposed-convolution">转置卷积（Transposed Convolution）<a href="#转置卷积transposed-convolution" class="hash-link" aria-label="转置卷积（Transposed Convolution）的直接链接" title="转置卷积（Transposed Convolution）的直接链接">​</a></h3><p>转置卷积是最常用的上采样层，有的时候也被称为“反卷积”或是“上卷积”。在输入和输出的关联关系上，转置卷积可以看作是卷积的一种反向操作，但实际上这并不是严格意义的逆运算，逆运算应该是可以被精确计算的，而转置卷积的计算结果并不是精确结果。转置卷积的一种示意如下图：</p><p><img loading="lazy" alt="image-20210502090940399" src="/assets/images/image-20210502090940399-adc90a6389322019772f510c437631a7.png" width="1348" height="616" class="img_ev3q"></p><p>如图，常见的转置卷积一般会通过某种方式在输入中填充0，以获得一张更大的特征图，其后使用一个标准的卷积运算获得一个比最初始的输入大一些的特征图作为输出。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="分解的转置卷积decomposed-transposed-convolution">分解的转置卷积（Decomposed transposed convolution）<a href="#分解的转置卷积decomposed-transposed-convolution" class="hash-link" aria-label="分解的转置卷积（Decomposed transposed convolution）的直接链接" title="分解的转置卷积（Decomposed transposed convolution）的直接链接">​</a></h3><p>分解的转置卷积和转置卷积是相似的：</p><p><img loading="lazy" alt="image-20210502091343561" src="/assets/images/image-20210502091343561-a8ad3fea12a2e87351daa223964f1f89.png" width="1320" height="626" class="img_ev3q"></p><p>只不过分解的转置卷积将主卷积运算分为多个低秩卷积。例如，在图像中，分解的转置卷积通过两个一维的卷积对二维的卷积进行模拟。例如上图中，对于输入，先在行上进行隔行填充，然后使用一维的卷积核进行卷积，再在列上进行隔列填充，再使用一维的卷积核进行卷积。</p><p>分解的转置卷积严格意义上是转置卷积的子集。</p><p><img loading="lazy" alt="image-20210502091954758" src="/assets/images/image-20210502091954758-839315277ea75c15e43f1fd629f750fe.png" width="1355" height="323" class="img_ev3q"></p><p>如上图，这样做的优势是降低了可训练变量的数量（降低了参数量）。</p><p>分解的转置卷积已经在inception结构中获得了成功：在ILSVRC2012分类赛中获得了the state of the art的成果。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="深度到空间的转换depth-to-space">深度到空间的转换（Depth-to-space）<a href="#深度到空间的转换depth-to-space" class="hash-link" aria-label="深度到空间的转换（Depth-to-space）的直接链接" title="深度到空间的转换（Depth-to-space）的直接链接">​</a></h3><p>这种方法（Depth to space）有时也被称为“subpixel convolution”的基本思路是将特征通道移入空间域：</p><p><img loading="lazy" alt="image-20210502092533184" src="/assets/images/image-20210502092533184-5196208527597e8530bc6cadc6dd2009.png" width="696" height="299" class="img_ev3q"></p><p>如上图，本应堆叠在channel维度的不同特征被融合进一个深度为1的平面特征图。这种方法能够很好地保留空间特征，因为它所做的仅仅是改变它们的位置而不是将它们堆叠进channel，而这种方法的缺点是引入了对齐伪像。为了能够和其他几个上采样方法进行横向对比，这篇论文在进行从深度到空间的转换实验之前的下采样卷积比其他上采样层的输出通道多了四倍。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="插值法interpolation">插值法（Interpolation）<a href="#插值法interpolation" class="hash-link" aria-label="插值法（Interpolation）的直接链接" title="插值法（Interpolation）的直接链接">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="最临近插值法nearest-interpolation">最临近插值法（Nearest Interpolation）<a href="#最临近插值法nearest-interpolation" class="hash-link" aria-label="最临近插值法（Nearest Interpolation）的直接链接" title="最临近插值法（Nearest Interpolation）的直接链接">​</a></h4><p><img loading="lazy" alt="image-20210502104856704" src="/assets/images/image-20210502104856704-820b42bb077b0fa6f6a615d4d202d044.png" width="1199" height="472" class="img_ev3q"></p><p>最近邻法不需要计算只需要寻找原图中对应的点，所以最近邻法速度最快，但是会破坏原图像中像素的渐变关系，原图像中的像素点的值是渐变的，但是在新图像中局部破坏了这种渐变关系。</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="线性插值法linear-interpolation">线性插值法（linear interpolation）<a href="#线性插值法linear-interpolation" class="hash-link" aria-label="线性插值法（linear interpolation）的直接链接" title="线性插值法（linear interpolation）的直接链接">​</a></h4><p>线性插值法（单线性插值法）是指使用连接两个已知量的直线来确定在这个两个已知量之间的一个未知量的值的方法。 </p><p><img loading="lazy" alt="image-20210502110046370" src="/assets/images/image-20210502110046370-642de9e036fa8b93984ba18e85218864.png" width="857" height="617" class="img_ev3q"></p><p>根据初中的知识，2点求一条直线公式，这是双线性插值所需要的唯一的基础公式。</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="双线性插值bilinear-interpolation">双线性插值（Bilinear interpolation）<a href="#双线性插值bilinear-interpolation" class="hash-link" aria-label="双线性插值（Bilinear interpolation）的直接链接" title="双线性插值（Bilinear interpolation）的直接链接">​</a></h4><p>双线性插值可以理解为进行了两次单线性插值：</p><p><img loading="lazy" alt="image-20210502110140524" src="/assets/images/image-20210502110140524-49147bc7ccbdfba3657f2899c6d3746c.png" width="848" height="650" class="img_ev3q"></p><p>先在x方向求2次单线性插值，获得R1(x, y1)、R2(x, y2)两个临时点，再在y方向计算1次单线性插值得出P(x, y)（实际上调换2次轴的方向先y后x也是一样的结果）</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="双线性上采样卷积bilinear-upsampling--convolution">双线性上采样+卷积（Bilinear upsampling + Convolution）<a href="#双线性上采样卷积bilinear-upsampling--convolution" class="hash-link" aria-label="双线性上采样+卷积（Bilinear upsampling + Convolution）的直接链接" title="双线性上采样+卷积（Bilinear upsampling + Convolution）的直接链接">​</a></h3><p>双线性上采样+卷积的意思就是在双线性插值之后进行卷积运算。为了和其他上采样方法比较，这篇论文中假设在上采样之后还要进行额外的卷积运算。这种方法的缺点是占用了大量内存和计算空间：双线性插值会二次增加特征量，但同时保持原来的“信息量”。由于假设了双线性上采样之后接有卷积运算，因此这种方法理论上比转置卷积方法的开销高四倍。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="双线性上采样可分离卷积bilinear-upsamplingseparable-convolution">双线性上采样+可分离卷积（Bilinear upsampling+Separable convolution）<a href="#双线性上采样可分离卷积bilinear-upsamplingseparable-convolution" class="hash-link" aria-label="双线性上采样+可分离卷积（Bilinear upsampling+Separable convolution）的直接链接" title="双线性上采样+可分离卷积（Bilinear upsampling+Separable convolution）的直接链接">​</a></h3><p>可分离的卷积用于构建简单且同质的网络结构，其结果优于InceptionV3。</p><p><img loading="lazy" alt="image-20210502111611626" src="/assets/images/image-20210502111611626-c4948cc58c8691b0b8b4931dcba4afd2.png" width="1317" height="901" class="img_ev3q"></p><p>如上图：一个可分离的卷积又两个操作组成：一个是对每个通道的卷积，另一个是使用<code>(1x1)</code>卷积核的逐点卷积对通道进行“混合”。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="双线性加性上采样bilinear-additive-upsampleing">双线性加性上采样（Bilinear additive upsampleing）<a href="#双线性加性上采样bilinear-additive-upsampleing" class="hash-link" aria-label="双线性加性上采样（Bilinear additive upsampleing）的直接链接" title="双线性加性上采样（Bilinear additive upsampleing）的直接链接">​</a></h3><p>这个方法是这篇论文在对上述现存的方法进行了叙述后提出的新方法。</p><p>该论文建议继续使用双线性上采样，但是该论文还将每N个连续的通道相加，从而将输出降低了N倍：</p><p><img loading="lazy" alt="image-20210502112224648" src="/assets/images/image-20210502112224648-4a2ee4f4a807de01686da164ca68dedd.png" width="1096" height="472" class="img_ev3q"></p><p>如上图，该方法的过程是确定性的，唯一可调的参数是N。虽然这个方法很像是之前说的“深度到空间的转换（Depth-to-space）”，但是这个方法并不会导致空间伪像的出现，也就是不需要考虑对齐操作。</p><p>在这篇论文的实验中，作者选择参数N的标准是让进行双线性加性上采样后和之前的浮点数相等，这使得这种上采样的性能开销类似于转置卷积。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="跨层连接和残差连接方法skip-connections-and-residual-connections">跨层连接和残差连接方法（Skip connections and residual connections）<a href="#跨层连接和残差连接方法skip-connections-and-residual-connections" class="hash-link" aria-label="跨层连接和残差连接方法（Skip connections and residual connections）的直接链接" title="跨层连接和残差连接方法（Skip connections and residual connections）的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="跨层连接skip-connections">跨层连接（Skip connections）<a href="#跨层连接skip-connections" class="hash-link" aria-label="跨层连接（Skip connections）的直接链接" title="跨层连接（Skip connections）的直接链接">​</a></h3><p>跨层连接有时也被叫做跳跃连接。这种方法已经在很多解码器结构中获得成功，并且在很多其他的计算机视觉任务中取得了不错的成绩。</p><p><img loading="lazy" alt="image-20210502113600719" src="/assets/images/image-20210502113600719-91c89d9feba894c0880df2b4d49250c7.png" width="1209" height="515" class="img_ev3q"></p><p>在这种方法中，解码器的每一层输入有两个来源：第一个是上层解码器得到的输出；第二个是在编码器中输出大小和自身输入大小匹配的一层输出的特征。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="解码器的残差连接residual-connections-for-decoders">解码器的残差连接（Residual connections for decoders）<a href="#解码器的残差连接residual-connections-for-decoders" class="hash-link" aria-label="解码器的残差连接（Residual connections for decoders）的直接链接" title="解码器的残差连接（Residual connections for decoders）的直接链接">​</a></h3><p>残差连接已经在很多不同的计算机视觉任务中被证明是有效的（来源是ResNet）。但是，残差连接并不能直接应用于解码器：在解码器中，下一层比上一层具有更大的空间分辨率和更少的通道数，这和起初残差被提出时的条件恰好相反。所以该论文提出了一个可以解决这些问题的转换方法：特别是上面提出的双线性加性上采样（Bilinear additive upsampleing）方法将输入转化为所需的空间大小和所需的通道数而无需提供任何额外的参数。其转化的特征包含了原始特征的很多信息。因此，可以使用这种转换方法（不进行额外的卷积）进行转换，然后将转换结果输入到任何上采样层的输出中作为下一个上采样层的输入，从而形成类似残差的连接：</p><p><img loading="lazy" alt="image-20210502114838945" src="/assets/images/image-20210502114838945-e53dc4e356c532efe2ea952b640b1475.png" width="1215" height="589" class="img_ev3q"></p><p>上图是对这种方法进行的图形化解释。在后面的内容中，这篇论文通过实验证明了这种方法的有效性。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="实验和实验设置task-and-experimental-setups">实验和实验设置（Task and experimental setups）<a href="#实验和实验设置task-and-experimental-setups" class="hash-link" aria-label="实验和实验设置（Task and experimental setups）的直接链接" title="实验和实验设置（Task and experimental setups）的直接链接">​</a></h2><p>实验部分请查看<a href="/papers/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.pdf">原论文</a>。</p></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/survey">survey</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/decoder">decoder</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/segmentation">segmentation</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.dev/neet-cv/ml.akasaki.space/blob/master/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="博文分页导航"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/[00]unlimited-paper-works"><div class="pagination-nav__sublabel">较新一篇</div><div class="pagination-nav__label">欢迎来到魔法部日志</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey"><div class="pagination-nav__sublabel">较旧一篇</div><div class="pagination-nav__label">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#相关研究related-works" class="table-of-contents__link toc-highlight">相关研究（Related works）</a></li><li><a href="#现存的上采样层设计existing-upsampling-layers" class="table-of-contents__link toc-highlight">现存的上采样层设计Existing upsampling layers）</a><ul><li><a href="#转置卷积transposed-convolution" class="table-of-contents__link toc-highlight">转置卷积（Transposed Convolution）</a></li><li><a href="#分解的转置卷积decomposed-transposed-convolution" class="table-of-contents__link toc-highlight">分解的转置卷积（Decomposed transposed convolution）</a></li><li><a href="#深度到空间的转换depth-to-space" class="table-of-contents__link toc-highlight">深度到空间的转换（Depth-to-space）</a></li><li><a href="#插值法interpolation" class="table-of-contents__link toc-highlight">插值法（Interpolation）</a></li><li><a href="#双线性上采样卷积bilinear-upsampling--convolution" class="table-of-contents__link toc-highlight">双线性上采样+卷积（Bilinear upsampling + Convolution）</a></li><li><a href="#双线性上采样可分离卷积bilinear-upsamplingseparable-convolution" class="table-of-contents__link toc-highlight">双线性上采样+可分离卷积（Bilinear upsampling+Separable convolution）</a></li><li><a href="#双线性加性上采样bilinear-additive-upsampleing" class="table-of-contents__link toc-highlight">双线性加性上采样（Bilinear additive upsampleing）</a></li></ul></li><li><a href="#跨层连接和残差连接方法skip-connections-and-residual-connections" class="table-of-contents__link toc-highlight">跨层连接和残差连接方法（Skip connections and residual connections）</a><ul><li><a href="#跨层连接skip-connections" class="table-of-contents__link toc-highlight">跨层连接（Skip connections）</a></li><li><a href="#解码器的残差连接residual-connections-for-decoders" class="table-of-contents__link toc-highlight">解码器的残差连接（Residual connections for decoders）</a></li></ul></li><li><a href="#实验和实验设置task-and-experimental-setups" class="table-of-contents__link toc-highlight">实验和实验设置（Task and experimental setups）</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">鲁ICP备2021025239号-2 Copyright © 2023 neet-cv.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.416d7f00.js"></script>
<script src="/assets/js/main.d42e6425.js"></script>
</body>
</html>