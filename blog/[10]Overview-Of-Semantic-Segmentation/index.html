<!doctype html>
<html lang="zh-cn" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">A Review on Deep Learning Techniques Applied to Semantic Segmentation | 工具箱的深度学习记事簿</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://ml.akasaki.space/blog/[10]Overview-Of-Semantic-Segmentation"><meta data-rh="true" name="docusaurus_locale" content="zh-cn"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="zh-cn"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="A Review on Deep Learning Techniques Applied to Semantic Segmentation | 工具箱的深度学习记事簿"><meta data-rh="true" name="description" content="这是一篇关于综述论文的解读。原论文（A Review on Deep Learning Techniques Applied to Semantic Segmentation）"><meta data-rh="true" property="og:description" content="这是一篇关于综述论文的解读。原论文（A Review on Deep Learning Techniques Applied to Semantic Segmentation）"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2023-12-31T09:31:53.000Z"><meta data-rh="true" property="article:author" content="https://gong.host"><meta data-rh="true" property="article:tag" content="survey,segmentation"><link data-rh="true" rel="icon" href="/img/logo.svg"><link data-rh="true" rel="canonical" href="https://ml.akasaki.space/blog/[10]Overview-Of-Semantic-Segmentation"><link data-rh="true" rel="alternate" href="https://ml.akasaki.space/blog/[10]Overview-Of-Semantic-Segmentation" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://ml.akasaki.space/blog/[10]Overview-Of-Semantic-Segmentation" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="工具箱的深度学习记事簿 RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="工具箱的深度学习记事簿 Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.3231e09c.css">
<link rel="preload" href="/assets/js/runtime~main.416d7f00.js" as="script">
<link rel="preload" href="/assets/js/main.d42e6425.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">工具箱的深度学习记事簿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">魔法部日志</a><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/about">Authors &amp; About</a><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">YetAnotherAkasaki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切换浅色/暗黑模式（当前为浅色模式）" aria-label="切换浅色/暗黑模式（当前为浅色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="搜索" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="最近博文导航"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[00]unlimited-paper-works">欢迎来到魔法部日志</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[06]DeepLab-Series">DeepLab Series</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks - A Survey</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution - Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering">PointRend - Image Segmentation as Rendering</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[20]Transformer-Attention-is-all-you-need">Transformer - Attention is all you need</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features">RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet - Low-Light Enhancement Network with Global Awareness</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[26]CBAM-Convolutional-Block-Attention-Module">CBAM - Convolutional Block Attention Module</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO - Vision Outlooker for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention - Towards High-quality Pixel-wise Regression</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">CCNet - Criss-Cross Attention for Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">RepVGG - Making VGG-style ConvNets Great Again</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">PP-LCNet - A Lightweight CPU Convolutional Neural Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">Swin Transformer - Hierarchical Vision Transformer using Shifted Windows</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[46]Demystifying-Local-Vision-Transformer">Demystifying Local Vision Transformer</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation">DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations">GhostNet - More Features from Cheap Operations</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer">Kindling the Darkness - A Practical Low-light Image Enhancer</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[51]How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode">How much Position Information Do Convolutional Neural Networks Encode?</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[52]Axiomatic-Attribution-for-Deep-Networks">Axiomatic Attribution for Deep Networks</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="title_f1Hy" itemprop="headline">A Review on Deep Learning Techniques Applied to Semantic Segmentation</h1><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 15 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | 键圈躺尸砖家</small></div></div></div></div></header><div id="post-content" class="markdown" itemprop="articleBody"><p>这是一篇关于综述论文的解读。<a href="https://arxiv.org/pdf/1704.06857.pdf" target="_blank" rel="noopener noreferrer">原论文（A Review on Deep Learning Techniques Applied to Semantic Segmentation）</a></p><p>摘要：</p><blockquote><p>Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.</p></blockquote><p>我看了这篇综述受益匪浅，如果有时间的话请阅读<a href="https://arxiv.org/pdf/1704.06857.pdf" target="_blank" rel="noopener noreferrer">原作</a>。本文只是对原作阅读的粗浅笔记。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="介绍分割">介绍分割<a href="#介绍分割" class="hash-link" aria-label="介绍分割的直接链接" title="介绍分割的直接链接">​</a></h2><p>对图像进行分割主要有：语义分割（Semantic segmentation）和实例分割（Instance segmentation）。它们的区别一目了然：</p><p><img loading="lazy" alt="image-20210427154733807" src="/assets/images/image-20210427154733807-39f0c9b4ad2e6cca317e8b6faf075822.png" width="1671" height="360" class="img_ev3q"></p><p>左图：原图；中图：语义分割；右图：实例分割。</p><p>很明显，语义分割希望将不同类别的物体所在位置的像素分开来，但是对于相同类别的不同物体并不敏感；而实例分割不但需要分开每一个位置上像素属于哪一类，还要分出它具体属于哪一个对象。</p><p>我们知道一个图像只不过是许多像素的集合。图像分割分类是对图像中属于特定类别的像素进行分类的过程，因此<strong>图像分割可以认为是按像素进行分类的问题</strong>。</p><p>如果你对离散数学以及softmax很敏感的化，肯定第一时间会产生这样的联想：</p><p><img loading="lazy" alt="image-20210427222245438" src="/assets/images/image-20210427222245438-149ef48133be5926deb58514f203e85c.png" width="1783" height="622" class="img_ev3q"></p><p>这张图实际上是这样的：</p><p><img loading="lazy" alt="image-20210427222340602" src="/assets/images/image-20210427222340602-84b084faa6a175bd4c5468bbc0261321.png" width="1245" height="704" class="img_ev3q"></p><p>当然，对于实际应用中通道数量的具体数字可根据实际需求选择。例如，在前景分割中，仅需分割出前景和背景，因此只需要一个通道。而全景分割中，如果使用类one-hot编码，则需要有和对象数目+1一样多的通道数。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="分割的技术">分割的技术<a href="#分割的技术" class="hash-link" aria-label="分割的技术的直接链接" title="分割的技术的直接链接">​</a></h2><p>在深度学习方法流行之前，TextonForest和基于随机森林分类器等语义分割方法是用得比较多的方法。但是本文章的背景是基于深度学习方法的计算机视觉，所以不做过多讨论。</p><p>深度学习技术在各个计算机领域获得了巨大的成功，其解决语义分割问题可以概括为几种思路：</p><ul><li>块分类（Patch classification）</li><li>全卷积方法（基于FCN）</li><li>编码器-解码器结构（encoder-decoder，本质基于FCN）</li><li>跨层连接的encoder-decoder结构</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="块分类patch-classification">块分类（Patch classification）<a href="#块分类patch-classification" class="hash-link" aria-label="块分类（Patch classification）的直接链接" title="块分类（Patch classification）的直接链接">​</a></h3><p>块分类算得上是一类最古老的方法。</p><p>如其名，把图像分成小块塞给网络进行分类。分成指定大小的小块是因为全连接网络只接受指定大小的输入。这大概是最初的基于深度学习的分割方法了（吧）。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="全卷积方法基于fcn">全卷积方法（基于FCN）<a href="#全卷积方法基于fcn" class="hash-link" aria-label="全卷积方法（基于FCN）的直接链接" title="全卷积方法（基于FCN）的直接链接">​</a></h3><p>全卷积方法在块分类之后，优势是使用全卷积代替了块分类中的全连接。</p><p>用于代替全连接的全卷积方法除了在其他视觉方法里很出名，也很快用到了分割算法中。2014年，全卷积网络（FCN）横空出世，FCN将网络全连接层用卷积取代，因此使任意图像大小的输入都变成可能，而且速度比Patch classification方法快很多。（我用简单分类模型实测了一下也是，全连接真的是太烂了，又慢又重，但是作为多层感知机到全卷积网路中间的过度组件，还是功不可没的。）</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="插值法实现的上采样">插值法实现的上采样<a href="#插值法实现的上采样" class="hash-link" aria-label="插值法实现的上采样的直接链接" title="插值法实现的上采样的直接链接">​</a></h4><p>在全卷积方法中，为了使输出和输入大小相同，在卷积导致特征图变小后还需要经过上采样使特征图变为原来大小。</p><p><img loading="lazy" alt="deconv01" src="/assets/images/deconv01-12cbc555dbcc715b8e007679f890b701.gif" width="344" height="386" class="img_ev3q"></p><p>上图：一种反卷积的示意。其中蓝色较小的特征图是输入，通过在它周围填充，使其变为较大的特征图后，再进行卷积。得到的结果是绿色的特征图。</p><p><img loading="lazy" alt="deconv02" src="/assets/images/deconv02-e9c47ef7664faf59e94f6cf0bb2916f0.gif" width="395" height="449" class="img_ev3q"></p><p>上图：另一种反卷积的示意。其中蓝色较小的特征图经过某种填充方法进行填充，变为较大的特征图后再进行卷积。</p><p>反卷积的常见思路是通过一些填充的方法将较小的特征图变大，然后通过卷积获得比原来的小特征图更大的特征图。较为常用的填充方法是插值法。</p><p>插值的方法主要可以分为两类，一类是线性图像插值方法：</p><ul><li>最近邻插值(Nearest neighbor interpolation)</li><li>双线性插值(Bi-Linear interpolation)</li><li>双立方插值(Bi-Cubic interpolation)</li></ul><p>另一类是非线性图像插值方法：</p><ul><li>基于小波变换的插值算法</li><li>基于边缘信息的插值算法。</li></ul><p>以上的这些方法都是一些插值方法，需要我们在决定网络结构的时候进行挑选。这些方法就像是人工特征工程一样，并没有给神经网络学习的余地，神经网络不能自己学习如何更好地进行插值，这个显然是不够理想的。</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="转置卷积实现的上采样">转置卷积实现的上采样<a href="#转置卷积实现的上采样" class="hash-link" aria-label="转置卷积实现的上采样的直接链接" title="转置卷积实现的上采样的直接链接">​</a></h4><p>在上采样的方法中，比较出名的是转置卷积，因为它允许我们使用可学习的上采样过程。</p><p>典型的转置卷积运算将采用滤波器视图中当前值的点积并作为相应的输出位置产生的单个值，而转置卷积的过程基本想法。对于转置卷积，我们从低分辨率特征图中获取单个值，并将滤波器中的所有权重乘以该值，将加权值输出到更大的特征图。</p><p><img loading="lazy" alt="image-20210427223356560" src="/assets/images/image-20210427223356560-eda2e0fa6f2471e9fb7ae85483e99f65.png" width="992" height="443" class="img_ev3q"></p><p>上图：转置卷积的一种示意。</p><blockquote><p>Tips：神经网络中的解卷积层也被称作：转置卷积(Transposed Convolution)、上卷积（upconvolution）、完全卷积（full convolution）、转置卷积（transposed convolution）、微步卷积（fractionally-strided convolution）。</p><p>转置卷积常常在一些文献中也称之为反卷积(Deconvolution)和部分跨越卷积(Fractionally-strided Convolution)，因为称之为反卷积容易让人以为和数字信号处理中反卷积混起来，造成不必要的误解，因此下文都将称为转置卷积，并且建议各位不要采用反卷积这个称呼。</p></blockquote><h3 class="anchor anchorWithStickyNavbar_LWe7" id="编码器-解码器结构encoder-decoder本质基于fcn">编码器-解码器结构（encoder-decoder，本质基于FCN）<a href="#编码器-解码器结构encoder-decoder本质基于fcn" class="hash-link" aria-label="编码器-解码器结构（encoder-decoder，本质基于FCN）的直接链接" title="编码器-解码器结构（encoder-decoder，本质基于FCN）的直接链接">​</a></h3><p>encoder由于pooling逐渐减少空间维度，而decoder逐渐恢复空间维度和细节信息。</p><p><img loading="lazy" alt="image-20210428220457279" src="/assets/images/image-20210428220457279-a78a6a13eebb96f40ea9a69600a225d0.png" width="943" height="307" class="img_ev3q"></p><p>实际上，符合下采样提取特征，再上采样恢复原大小的都可以称为encoder-decoder结构。</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="跨层连接的encoder-decoder结构">跨层连接的encoder-decoder结构<a href="#跨层连接的encoder-decoder结构" class="hash-link" aria-label="跨层连接的encoder-decoder结构的直接链接" title="跨层连接的encoder-decoder结构的直接链接">​</a></h4><p>通常从encoder到decoder还有shortcut connetction（捷径连接，也就是跨层连接，其思想我猜是从VGG跨层连接出现的思想）。</p><p><img loading="lazy" alt="image-20210427221642324" src="/assets/images/image-20210427221642324-a345eb6ead9c3b337830854dd9f673fb.png" width="1857" height="775" class="img_ev3q"></p><p>上图是带有跨层连接的encoder-decoder的代表之一：UNet的结构。</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="高低层特征融合">高低层特征融合<a href="#高低层特征融合" class="hash-link" aria-label="高低层特征融合的直接链接" title="高低层特征融合的直接链接">​</a></h4><p>由于池化操作造成的信息损失，上采样（即使采用解卷积操作）只能生成粗略的分割结果图。因此，论文从高分辨率的特征图中引入跳跃连接（shortcut/skip connection）操作改善上采样的精细程度（感觉像是从ResNet开始出现的思想）：</p><p><img loading="lazy" alt="FCN-2" src="/assets/images/FCN-2-2ea09f8a6d42611290674a123976c76e.png" width="1405" height="358" class="img_ev3q"></p><p>实验表明，这样的分割结果更细致更准确。在逐层fusion的过程中，做到第三行再往下，结果又会变差，所以作者做到这里就停了。可以看到如上三行的对应的结果：</p><p><img loading="lazy" alt="FCN-3" src="/assets/images/FCN-3-ab6307d70d4e66cf08c57096a9485edc.png" width="1260" height="539" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="空洞卷积dilatedatrous-convolution代替了池化-上采样的过程">空洞卷积（Dilated/Atrous Convolution，代替了“池化-上采样”的过程）<a href="#空洞卷积dilatedatrous-convolution代替了池化-上采样的过程" class="hash-link" aria-label="空洞卷积（Dilated/Atrous Convolution，代替了“池化-上采样”的过程）的直接链接" title="空洞卷积（Dilated/Atrous Convolution，代替了“池化-上采样”的过程）的直接链接">​</a></h3><p>尽管FCN及encoder-decoder结构中移除了全连接层，但是CNN模型用于语义分割还存在一个问题，就是下采样操作。这里使用池化的下采样为例：pooling操作可以扩大感受野因而能够很好地整合上下文信息（context中文称为语境或者上下文，通俗的理解就是综合了更多的信息来进行决策），对high-level的任务（比如分类），这是很有效的。但同时，由于pooling下采样操作，使得分辨率降低，因此削弱了位置信息，而语义分割中需要score map和原图对齐，因此需要丰富的位置信息。</p><p>Dilated/Atrous Convolution（空洞卷积），这种结构代替了池化，一方面它可以保持空间分辨率，另外一方面它由于可以扩大感受野因而可以很好地整合上下文信息（我觉得这个设计很有意思，原图的大小完全不会改变，也不需要上采样了）。</p><p><img loading="lazy" alt="image-20210427221923919" src="/assets/images/image-20210427221923919-66e60b92bfcda45838fac56515297bfd.png" width="832" height="484" class="img_ev3q"></p><p>上图：在某篇论文中出现的空洞卷积示意图。</p><p><img loading="lazy" alt="Atrous_conv" src="/assets/images/Atrous_conv-8bbc9b569825f67962e113f732b01b61.png" width="1423" height="675" class="img_ev3q"></p><p>上图：另一张空洞卷积的示意图。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="条件随机场">条件随机场<a href="#条件随机场" class="hash-link" aria-label="条件随机场的直接链接" title="条件随机场的直接链接">​</a></h3><p>在使用全卷积网络的分割方法中，有一个很常用的基本框架：</p><p><img loading="lazy" alt="img" src="/assets/images/CRF01-f823011d924cce31a8c37c26aff7d561.jpg" width="720" height="286" class="img_ev3q"></p><p>其中， FCN 表示各种全卷积网络，CRF 为条件随机场，MRF 为马尔科夫随机场。其大致思路就是前端使用 FCN 进行特征粗提取，后端使用 CRF/MRF 优化前端的输出，最后得到分割图。</p><p><a href="https://arxiv.org/pdf/1210.5644.pdf" target="_blank" rel="noopener noreferrer">条件随机场（Conditional Random Field，CRF）</a> 后处理操作通常用于进一步改善分割的效果。CRFs 是一种基于底层图像的像素强度进行“平滑”分割（‘smooth’ segmentation）的图模型，其工作原理是相似强度的像素更可能标记为同一类别。CRFs 一般能够提升 1-2% 的精度。</p><p><img loading="lazy" alt="CRF" src="/assets/images/CRF-624c9c654b86b81285c6a4f7a2c1b892.png" width="1240" height="603" class="img_ev3q"></p><p>上图为CRF示意图。（b）一元分类结合CRF;（c, d, e）是CRF的变体，其中(e)是广泛使用的一种CRF。</p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="分割的数据集">分割的数据集<a href="#分割的数据集" class="hash-link" aria-label="分割的数据集的直接链接" title="分割的数据集的直接链接">​</a></h2><p>截止到原综述写作时间为止时较为流行的数据集：</p><p><img loading="lazy" alt="image-20210428094548476" src="/assets/images/image-20210428094548476-a29569648f65f36a58b06b15f8f24bc9.png" width="2184" height="748" class="img_ev3q"></p><p>还没看完，看完就写。</p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="领域知名论文">领域知名论文<a href="#领域知名论文" class="hash-link" aria-label="领域知名论文的直接链接" title="领域知名论文的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="基于深度学习的分割方法">基于深度学习的分割方法<a href="#基于深度学习的分割方法" class="hash-link" aria-label="基于深度学习的分割方法的直接链接" title="基于深度学习的分割方法的直接链接">​</a></h3><p><img loading="lazy" alt="image-20210428094705161" src="/assets/images/image-20210428094705161-bfe0531f2b6fbf9c4e2820660c89f60b.png" width="2100" height="686" class="img_ev3q"></p><ol><li><p>FCN</p><p>主要贡献：使端对端的卷积语义分割网络变得流行起来；通过deconvolutional layers进行上采样；通过skip connection改善了上采样的粗糙度。</p></li><li><p>SegNet</p><p>主要贡献：使用Maxpooling indices来增强位置信息。</p></li><li><p>Dilated Convolutions</p><p>主要贡献：使用空洞卷积用来进行稠密预测（dense prediction）；提出上下文模块（context module），使用空洞卷积（Dilated Convolutions）来进行多尺度信息的的整合。</p></li><li><p>DeepLab (v1 &amp; v2)</p><p>主要贡献：使用atrous卷积，也就是后来的空洞卷积，扩大感受野，保持分辨率；提出了atrous spatial pyramid pooling (ASPP)，整合多尺度信息；使用全连接条件随机场（fully connected CRF)进行后处理，改善分割结果。</p></li><li><p>RefineNet</p><p>主要贡献：精心设计了encoder-decoder架构中的decoder部分，使得性能提升；整个网络的设计都遵循residual connections，网络表达能力更强，梯度更容易反向传播。</p></li><li><p>PSPNet</p><p>主要贡献：使用pyramid pooling整合context；使用auxiliary loss。</p></li><li><p>Large Kernel Matters</p><p>主要贡献：提出一种具有非常大的内核卷积的编码器-解码器体系结构。</p></li><li><p>DeepLab v3</p><p>主要贡献：改进的无孔空间金字塔池化（ASPP）；级联使用atrous卷积的模块。</p></li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="上述方法的关系">上述方法的关系<a href="#上述方法的关系" class="hash-link" aria-label="上述方法的关系的直接链接" title="上述方法的关系的直接链接">​</a></h3><p><img loading="lazy" alt="image-20210428094839526" src="/assets/images/image-20210428094839526-9350dccfcbd7ce8dc703d2d73ace40cd.png" width="2055" height="996" class="img_ev3q"></p></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/survey">survey</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/segmentation">segmentation</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.dev/neet-cv/ml.akasaki.space/blob/master/blog/[10]Overview-Of-Semantic-Segmentation.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="博文分页导航"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection"><div class="pagination-nav__sublabel">较新一篇</div><div class="pagination-nav__label">Feature Pyramid Networks for Object Detection</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck"><div class="pagination-nav__sublabel">较旧一篇</div><div class="pagination-nav__label">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#介绍分割" class="table-of-contents__link toc-highlight">介绍分割</a></li><li><a href="#分割的技术" class="table-of-contents__link toc-highlight">分割的技术</a><ul><li><a href="#块分类patch-classification" class="table-of-contents__link toc-highlight">块分类（Patch classification）</a></li><li><a href="#全卷积方法基于fcn" class="table-of-contents__link toc-highlight">全卷积方法（基于FCN）</a></li><li><a href="#编码器-解码器结构encoder-decoder本质基于fcn" class="table-of-contents__link toc-highlight">编码器-解码器结构（encoder-decoder，本质基于FCN）</a></li><li><a href="#空洞卷积dilatedatrous-convolution代替了池化-上采样的过程" class="table-of-contents__link toc-highlight">空洞卷积（Dilated/Atrous Convolution，代替了“池化-上采样”的过程）</a></li><li><a href="#条件随机场" class="table-of-contents__link toc-highlight">条件随机场</a></li></ul></li><li><a href="#分割的数据集" class="table-of-contents__link toc-highlight">分割的数据集</a></li><li><a href="#领域知名论文" class="table-of-contents__link toc-highlight">领域知名论文</a><ul><li><a href="#基于深度学习的分割方法" class="table-of-contents__link toc-highlight">基于深度学习的分割方法</a></li><li><a href="#上述方法的关系" class="table-of-contents__link toc-highlight">上述方法的关系</a></li></ul></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">鲁ICP备2021025239号-2 Copyright © 2023 neet-cv.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.416d7f00.js"></script>
<script src="/assets/js/main.d42e6425.js"></script>
</body>
</html>