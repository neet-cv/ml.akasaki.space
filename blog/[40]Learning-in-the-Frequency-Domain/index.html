<!doctype html>
<html lang="zh-cn" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">Learning in the Frequency Domain | 工具箱的深度学习记事簿</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://ml.akasaki.space/blog/[40]Learning-in-the-Frequency-Domain"><meta data-rh="true" name="docusaurus_locale" content="zh-cn"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="zh-cn"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Learning in the Frequency Domain | 工具箱的深度学习记事簿"><meta data-rh="true" name="description" content="Kai Xu, Minghai Qin, Fei Sun, Yuhao Wang, Yen-Kuang Chen, Fengbo Ren"><meta data-rh="true" property="og:description" content="Kai Xu, Minghai Qin, Fei Sun, Yuhao Wang, Yen-Kuang Chen, Fengbo Ren"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2023-12-31T09:31:53.000Z"><meta data-rh="true" property="article:author" content="https://gong.host"><meta data-rh="true" property="article:tag" content="frequency-domain,attention-mechanism,non-convolution"><link data-rh="true" rel="icon" href="/img/logo.svg"><link data-rh="true" rel="canonical" href="https://ml.akasaki.space/blog/[40]Learning-in-the-Frequency-Domain"><link data-rh="true" rel="alternate" href="https://ml.akasaki.space/blog/[40]Learning-in-the-Frequency-Domain" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://ml.akasaki.space/blog/[40]Learning-in-the-Frequency-Domain" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="工具箱的深度学习记事簿 RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="工具箱的深度学习记事簿 Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.3231e09c.css">
<link rel="preload" href="/assets/js/runtime~main.416d7f00.js" as="script">
<link rel="preload" href="/assets/js/main.d42e6425.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">工具箱的深度学习记事簿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">魔法部日志</a><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/about">Authors &amp; About</a><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">YetAnotherAkasaki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切换浅色/暗黑模式（当前为浅色模式）" aria-label="切换浅色/暗黑模式（当前为浅色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="搜索" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="最近博文导航"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[00]unlimited-paper-works">欢迎来到魔法部日志</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[06]DeepLab-Series">DeepLab Series</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks - A Survey</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution - Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering">PointRend - Image Segmentation as Rendering</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[20]Transformer-Attention-is-all-you-need">Transformer - Attention is all you need</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features">RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet - Low-Light Enhancement Network with Global Awareness</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[26]CBAM-Convolutional-Block-Attention-Module">CBAM - Convolutional Block Attention Module</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO - Vision Outlooker for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention - Towards High-quality Pixel-wise Regression</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">CCNet - Criss-Cross Attention for Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">RepVGG - Making VGG-style ConvNets Great Again</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">PP-LCNet - A Lightweight CPU Convolutional Neural Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">Swin Transformer - Hierarchical Vision Transformer using Shifted Windows</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[46]Demystifying-Local-Vision-Transformer">Demystifying Local Vision Transformer</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation">DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations">GhostNet - More Features from Cheap Operations</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer">Kindling the Darkness - A Practical Low-light Image Enhancer</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[51]How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode">How much Position Information Do Convolutional Neural Networks Encode?</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[52]Axiomatic-Attribution-for-Deep-Networks">Axiomatic Attribution for Deep Networks</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="title_f1Hy" itemprop="headline">Learning in the Frequency Domain</h1><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 21 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | 键圈躺尸砖家</small></div></div></div></div></header><div id="post-content" class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu%2C+K" target="_blank" rel="noopener noreferrer">Kai Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qin%2C+M" target="_blank" rel="noopener noreferrer">Minghai Qin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun%2C+F" target="_blank" rel="noopener noreferrer">Fei Sun</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+Y" target="_blank" rel="noopener noreferrer">Yuhao Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen%2C+Y" target="_blank" rel="noopener noreferrer">Yen-Kuang Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ren%2C+F" target="_blank" rel="noopener noreferrer">Fengbo Ren</a></p><blockquote><p>  Deep neural networks have achieved remarkable success in computer vision tasks. Existing neural networks mainly operate in the spatial domain with fixed input sizes. For practical applications, images are usually large and have to be downsampled to the predetermined input size of neural networks. Even though the downsampling operations reduce computation and the required communication bandwidth, it removes both redundant and salient information obliviously, which results in accuracy degradation. Inspired by digital signal processing theories, we analyze the spectral bias from the frequency perspective and propose a learning-based frequency selection method to identify the trivial frequency components which can be removed without accuracy loss. The proposed method of learning in the frequency domain leverages identical structures of the well-known neural networks, such as ResNet-50, MobileNetV2, and Mask R-CNN, while accepting the frequency-domain information as the input. Experiment results show that learning in the frequency domain with static channel selection can achieve higher accuracy than the conventional spatial downsampling approach and meanwhile further reduce the input data size. Specifically for ImageNet classification with the same inpu t size, the proposed method achieves 1.41% and 0.66% top-1 accuracy improvements on ResNet-50 and MobileNetV2, respectively. Even with half input size, the proposed method still improves the top-1 accuracy on ResNet-50 by 1%. In addition, we observe a 0.8% average precision improvement on Mask R-CNN for instance segmentation on the COCO dataset.</p></blockquote><p><code>Comments</code>: Accepted to CVPR 2020</p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="概览">概览<a href="#概览" class="hash-link" aria-label="概览的直接链接" title="概览的直接链接">​</a></h2><p>在传统的CNN结构中，受限于设备的计算性能（也有的时候单纯是为了产生固定长度的输出以适应需要固定大小输入的全连接层），通常会使用固定大小的图像作为输入（例如：224<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>×</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">\times224</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">×</span><span class="mord">224</span></span></span></span></span>），因此通常会先将图片下采样到某个大小。这已经不能满足现代摄像机实际能达到的视觉画面精度了。加之传统CNN通常在空间上对图像进行操作，这样的行为有意无意中将图像的部分高频信息以及显著性信息移除，直接或间接地导致图像信息或精度的丢失（参考<a href="https://arxiv.org/abs/1810.05552" target="_blank" rel="noopener noreferrer">Effects of Image Degradations to CNN-based Image Classification</a>）。这篇文章在Introduction中提到以前的工作通过对特定的视觉任务控制这个downsample的过程来降低信息损失（例如 <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Heewon_Kim_Task-Aware_Image_Downscaling_ECCV_2018_paper.pdf" target="_blank" rel="noopener noreferrer">Task-Aware Image Downscaling</a>）。</p><p>本文的作者受到HVS（human visual system，人类视觉系统）中的一些研究（<a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Kim_Deep_Learning_of_CVPR_2017_paper.pdf" target="_blank" rel="noopener noreferrer">Deep Learning of Human Visual Sensitivity in Image Quality Assessment Framework</a>）以及数字信号处理中一些方法的启发，首先将图像通过某种方法映射到频域内（例如离散余弦变换（discrete cosine transform）或其他方式），获得频域上的特征图后，训练一种“选择器”，其功能是筛选出对最终结果影响较大的频域信息，并移除无关紧要的部分，作为输入后续网络的数据的一部分。</p><p><img loading="lazy" alt="image-20211023123854207" src="/assets/images/image-20211023123854207-143552f9c0bea6ac8cfcc9dd3525e975.png" width="1306" height="541" class="img_ev3q"></p><p>上图(a)：一般CNN模型处理图片的流程；上图(b)：本文的方法处理图片的流程（值得一提的是，该方法作为前处理加入网络时，不需要对原来使用RGB图像作为输入的CNN结构做出什么更改）。实验证明，在相同的输入图像精度下，加入该方法的神经网络比直接在空间域上卷积完成的逐层下采样的方法达到了更高的精度，并且在二分之一输入大小的图像上依然如此。	</p><p>从代码上看，本篇论文的贡献主要是一个频域的前处理过程。作者在论文中说明了，将输入转换进频域的前处理几乎可以直接用于任何CNN模型。</p><blockquote><p>  关注到本文，是因为最近研究分割比较多，并且我觉得分割作为一个逐像素的密集预测问题，其所需的编码方式和类别数量巨大的分类问题以及目标检测问题可以分开来讲。抛开实例分割这种除了分类还需要区分实体的人物不谈，在单纯的语义分割中，分类图像的方式基本是通过编码后特征图每个像素对应channel上解码出的概率分布。在分类数量较少时，我认为所需要的编码复杂度并不需要很高。降低编码的复杂度，一方面能够加快参数收敛的速度，另一方面能够降低网络参数两。自从拜读了一些通道注意力相关的文章（例如<a href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze and Excitation Networks</a>），我就隐约感到CNN在较长的channel上可能会隐式地编码一些频率信息，只是由于在网络的监督上没有针对这一点进行优化，所以表现得并不明显。此篇文章提出在频域上选择必要的信息，并不是像我想的那样在直接在监督上使网络显式编码频率信息，而是通过前处理使网络在输入上就表现出对频域的偏置。这和我一开始的想法有些出入，非常具有启发意义。</p></blockquote><p>本文的主要贡献如下：</p><ol><li>本文提出了一种不需要更改原有CNN网络结构的频域学习方法，其主要使用“某种频域像关系数”（DCT coefficients as input）作为输入而不是RGB图像。该方法在许多视觉任务上具有出色表现。</li><li>本文通过实验证明，使用在频域上的前处理比直接使用卷积操作进行下采样更能保留图像的信息。</li><li>本文从频域的角度对“光谱偏置”进行了分析并证明了使用RGB图像作为输入的CNN模型对低频信息更加敏感，这和人类的视觉系统表现是贴合的。</li><li>本文提出了一种可学习的动态channel选择器。该选择器通过学习在推理时静态地移除一部分相关性不大的频率分量（通过拦截部分通道）。</li></ol><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="提出了频域的数据前处理过程">提出了频域的数据前处理过程<a href="#提出了频域的数据前处理过程" class="hash-link" aria-label="提出了频域的数据前处理过程的直接链接" title="提出了频域的数据前处理过程的直接链接">​</a></h2><p>很少有CNN模型会直接将高清图片直接放入GPU或任何计算设备中进行推理。这会导致设备内存、带宽或计算瓶颈。为了解决这个问题，高清图片通常被CPU提前处理成较小的图片输入进网络。这通常会导致一些信息的损失以及精度下降。</p><p>本文中的方法是将RGB色域的图像转换进YCbCr颜色空间，再转换进频域空间。</p><blockquote><p>  YCbCr或Y&#x27;CbCr有的时候会被写作：YCBCR或是Y&#x27;CBCR，是<a href="https://zh.wikipedia.org/wiki/%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%96%93" target="_blank" rel="noopener noreferrer">色彩空间</a>的一种，通常会用于影片中的影像连续处理，或是数字摄影系统中。Y&#x27;和Y是不同的，Y就是所谓的<a href="https://zh.wikipedia.org/wiki/%E6%B5%81%E6%98%8E" target="_blank" rel="noopener noreferrer">流明</a>（<a href="https://zh.wikipedia.org/w/index.php?title=Luminance&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener noreferrer">luminance</a>），Y&#x27;表示光的浓度且为非线性，使用<a href="https://zh.wikipedia.org/wiki/%E4%BC%BD%E7%91%AA%E6%A0%A1%E6%AD%A3" target="_blank" rel="noopener noreferrer">伽马修正</a>（gamma correction）编码处理，而CB和CR则为蓝色和红色的浓度偏移量成分。</p><p>  <strong>YCbCr</strong>不是一种<a href="https://zh.wikipedia.org/wiki/%E7%B5%95%E5%B0%8D%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%96%93" target="_blank" rel="noopener noreferrer">绝对色彩空间</a>，是<a href="https://zh.wikipedia.org/wiki/YUV" target="_blank" rel="noopener noreferrer">YUV</a>压缩和偏移的版本。YCbCr的Y与YUV中的Y含义一致，Cb和Cr与UV同样都指色彩，Cb指蓝色色度，Cr指红色色度，在应用上很广泛，JPEG、MPEG、DVD、<a href="https://zh.wikipedia.org/wiki/%E6%94%9D%E5%BD%B1%E6%A9%9F" target="_blank" rel="noopener noreferrer">摄影机</a>、<a href="https://zh.wikipedia.org/wiki/%E6%95%B8%E4%BD%8D%E9%9B%BB%E8%A6%96" target="_blank" rel="noopener noreferrer">数字电视</a>等皆采此一格式。因此<strong>一般俗称的YUV大多是指YCbCr</strong>。</p><p>  <strong>YUV</strong>，是一种<a href="https://zh.wikipedia.org/wiki/%E9%A1%8F%E8%89%B2" target="_blank" rel="noopener noreferrer">颜色</a><a href="https://zh.wikipedia.org/wiki/%E7%B7%A8%E7%A2%BC" target="_blank" rel="noopener noreferrer">编码</a>方法。常使用在各个影像处理组件中。 YUV在对照片或视频编码时，考虑到人类的感知能力，允许降低色度的带宽。YUV是编译true-color颜色空间（color space）的种类，Y&#x27;UV, YUV, <a href="https://zh.wikipedia.org/wiki/YCbCr" target="_blank" rel="noopener noreferrer">YCbCr</a>，<a href="https://zh.wikipedia.org/wiki/YPbPr" target="_blank" rel="noopener noreferrer">YPbPr</a>等专有名词都可以称为YUV，彼此有重叠。“Y”表示<strong><a href="https://zh.wikipedia.org/wiki/%E6%B5%81%E6%98%8E" target="_blank" rel="noopener noreferrer">明亮度</a></strong>（Luminance、Luma），“U”和“V”则是<strong><a href="https://zh.wikipedia.org/wiki/%E8%89%B2%E5%BA%A6_(%E8%89%B2%E5%BD%A9%E5%AD%A6)" target="_blank" rel="noopener noreferrer">色度</a></strong>、<strong><a href="https://zh.wikipedia.org/wiki/%E6%BF%83%E5%BA%A6_(%E8%89%B2%E5%BD%A9%E5%AD%B8)" target="_blank" rel="noopener noreferrer">浓度</a></strong>（Chrominance、Chroma）</p></blockquote><p>上述内容摘自 Wikipedia 中关于 <a href="https://zh.wikipedia.org/wiki/YCbCr" target="_blank" rel="noopener noreferrer">YCbCr</a> 以及 <a href="https://zh.wikipedia.org/wiki/YUV" target="_blank" rel="noopener noreferrer">YUV </a>的相关内容。本篇论文的方法中没有直接将RGB色域的图像转化进频域空间，而是将数据先放入YCbCr颜色空间是有理由的。也许你会在多媒体技术或者数字图像处理课程上了解过早期电视机的相关内容，在该颜色空间下黑白视频是只有Y（Luma，Luminance）通道的视频，也就是灰阶值。到了彩色电视规格的制定，是以YUV/<a href="https://zh.wikipedia.org/wiki/YIQ" target="_blank" rel="noopener noreferrer">YIQ</a>的格式来处理彩色电视图像，把UV视作表示彩度的C（Chrominance或Chroma），如果忽略C信号，那么剩下的Y（Luma）信号就跟之前的黑白电视频号相同，这样一来便解决彩色电视机与黑白电视机的兼容问题。Y&#x27;UV最大的优点在于只需占用极少的带宽。</p><p>刚才提到“HVS中的一些研究”，作者在频域中对CNN的输入输出进行测试，通过实验分析发现，在分类、检测和分割任务中，CNN模型对低频率的channel更加敏感。这和HVS中的一些研究是贴合的。也就是说，在使用现在主流的数据集进行监督时，CNN在频域上表现出了和人类一样的“低频敏感性”。因此，对于RGB色域上的输入，并不是整个RGB空间内的所有值在CNN模型中都具有重要的作用。</p><p><img loading="lazy" alt="image-20211023123921777" src="/assets/images/image-20211023123921777-7e1ba4d868642adaf13aa81dce110086.png" width="1490" height="559" class="img_ev3q"></p><p>上图：在本论文提出的方法中图像前处理的过程。该过程包含如下步骤：</p><ol><li>Spatial resize and crop：将图像转为输入大小。</li><li>DCT transform：将图像从RGB色域转入YCbCr颜色空间，再通过DCT变换转入频域。</li><li>DCT channel select：一个选择器选择的过程，选出通道中对推理具有更大影响的部分。</li><li>DCT concatenate：将刚才在YCbCr的三个通道里做出的频域选择结果拼接成一个张量。</li><li>DCT normalize：利用训练数据集计算的均值和方差对每个频率通道进行归一化处理。</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="提出频域的通道选择器">提出频域的通道选择器<a href="#提出频域的通道选择器" class="hash-link" aria-label="提出频域的通道选择器的直接链接" title="提出频域的通道选择器的直接链接">​</a></h2><p>在聊频域通道的选择（原文：Frequency Channel Selection）之前，先回忆起 <a href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze and Excitation Networks</a> 中使用Excitation的过程选择channel，在频域的选择问题上，也许可以使用相似的思路。例如，将不同的频域信息堆叠在channel中，使用类似Squeeze的过程产生一组等于channel长度的选择器，决定某个channel是否进入输出。在CNN中是存在对光谱的偏置的（主要体现为输入为RGB图像），因此对于编码好的特征图，可以在channel中选出“比较重要的部分”。</p><blockquote><p>  刚才提到“HVS中的一些研究”，作者在频域中对CNN的输入输出进行测试，通过实验分析发现，在分类、检测和分割任务中，CNN模型对低频率的channel更加敏感。这和HVS中的一些研究是贴合的。也就是说，在使用现在主流的数据集进行监督时，CNN在频域上表现出了和人类一样的“低频敏感性”。</p></blockquote><p>基于上述思考，我猜对于频域上的特征图这样做的理由也是相同存在的。还真猜对了，在经过前处理后，特征图上不同的channel堆叠了不同的频域信息。本篇论文中提出通过学习筛除一部分对最终的结果及误差影响不大的通道。在刚才的前处理中，图像变为了<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">W\times H\times C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span></span></span></span></span>（本文中<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>=</mo><mn>192</mn></mrow><annotation encoding="application/x-tex">C=192</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">192</span></span></span></span></span>）的频域特征图，输入频率选择器。这篇文章在通道选择上直接使用了类似SE Block的结构：</p><p><img loading="lazy" alt="image-20211023123801381" src="/assets/images/image-20211023123801381-c638ae94a7427fe2325aa8e63a585b76.png" width="1495" height="710" class="img_ev3q"></p><p>上图：选择器结构（下，文中称之为 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>a</mi><mi>t</mi><mi>e</mi><mtext> </mtext><mi>M</mi><mi>o</mi><mi>d</mi><mi>u</mi><mi>l</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">Gate\ Module</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">G</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mspace"> </span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">o</span><span class="mord mathnormal">d</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">e</span></span></span></span></span>）和SE Block（上）的结构对比图。可以看到，除了图中标号1和2，其余部分和两者结构基本相同（注：本文的<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mrow><mi>e</mi><mi>x</mi></mrow></msub><mo stretchy="false">(</mo><mo>⋅</mo><mo separator="true">,</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F_{ex}(\cdot,W)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">⋅</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mclose">)</span></span></span></span></span>过程是<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span>卷积，而不是SE Block中全连接）。在Gate Module的输出中标记为白色的通道代表被过滤的通道（查看<a href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze and Excitation Networks</a> 以理解 SE Block ）。</p><p>论文的作者在文中说明了该结构与SE Block的不同之处：在 SE Block 中，对channel的采样是数值的，每个channel会得到一个数值的权重。而本文的Gate Module则对channel进行 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">0-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">0</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span> 采样。采样的方式是 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>o</mi><mi>r</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">Tensor 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.02778em">sor</span><span class="mord">3</span></span></span></span></span> 通过两套参数变为 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>o</mi><mi>r</mi><mn>4</mn></mrow><annotation encoding="application/x-tex">Tensor 4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.02778em">sor</span><span class="mord">4</span></span></span></span></span>（shape为<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn><mo>×</mo><mi>C</mi><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">1\times 1\times C\times 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">2</span></span></span></span></span>），然后再进行伯努利分布的采样。原文中是这样描述的：</p><blockquote><p>  The decision is obtained by sampling a Bernoulli distribution <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>e</mi><mi>r</mi><mi>n</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Bern(p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mord mathnormal" style="margin-right:0.02778em">er</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span></span>, where <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span></span></span></span></span> is calculated by the 2 numbers in the <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn><mo>×</mo><mi>C</mi><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">1 \times 1 \times C \times 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">2</span></span></span></span></span> tensor mentioned above.</p></blockquote><p>在原SE Block的设计中经过“<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mrow><mi>e</mi><mi>x</mi></mrow></msub></mrow><annotation encoding="application/x-tex">F_{ex}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>”过程得到 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>o</mi><mi>r</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">Tensor 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.02778em">sor</span><span class="mord">3</span></span></span></span></span> 后直接 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">softmax</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span></span></span></span></span> 。但是在本文给出的Gate Module中产生了一个问题。上面描述的选择方法中使用了伯努利分布采样，这会产生离散的通道选择决策。当我们采用梯度下降优化时，能够直接优化的是连续量。对于离散量以及在中间过程中出现离散量的网络是难以直接进行梯度下降优化的。因此，在Gate Module中，然后再进行 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>l</mi><mtext> </mtext><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">Gumbel\ softmax</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal">G</span><span class="mord mathnormal">u</span><span class="mord mathnormal">mb</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mspace"> </span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span></span></span></span></span>（ Gumbel softmax distribution 可参考论文<a href="https://arxiv.org/abs/1611.01144" target="_blank" rel="noopener noreferrer">Categorical Reparameterization with Gumbel-Softmax </a>）。<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>l</mi><mtext> </mtext><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">Gumbel\ softmax</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal">G</span><span class="mord mathnormal">u</span><span class="mord mathnormal">mb</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mspace"> </span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span></span></span></span></span> 允许了在具有离散采样的过程中进行反向传播，解决了这个问题。</p><p><img loading="lazy" alt="image-20211023123813633" src="/assets/images/image-20211023123813633-4e562f4018f87b52b0d8104f682b4686.png" width="660" height="257" class="img_ev3q"></p><p>上图：通过类似 SE Block 的结构选取重要的通道。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="探究哪些通道被选择了">探究哪些通道被选择了<a href="#探究哪些通道被选择了" class="hash-link" aria-label="探究哪些通道被选择了的直接链接" title="探究哪些通道被选择了的直接链接">​</a></h2><p>为了研究通道选择器的行为，作者使用相同数据集在图像分类和语义分割任务上进行训练，并对频域的选择器选出的channel信息进行了可视化：</p><p><img loading="lazy" alt="image-20211023123725921" src="/assets/images/image-20211023123725921-243d8fd9635d53cb6001654b2c2ae5eb.png" width="1010" height="334" class="img_ev3q"></p><p>上图：在 ImageNet（validation set）进行<strong>分类</strong>任务时对选出的通道 YCbCr 组成绘制可视化 heat map。</p><p><img loading="lazy" alt="image-20211023123729829" src="/assets/images/image-20211023123729829-4253f019480251812a84a158bd7b92bf.png" width="1010" height="334" class="img_ev3q"></p><p>上图：在 COCO（validation set）进行<strong>分割</strong>任务时对选出的通道 YCbCr 组成绘制可视化 heat map。</p><blockquote><p>  阅读上述 heat map 的方法：在 heat map 中每个代表channel方块上具有的index编号表示该通道对应的frequency index。颜色越深表示在前向传播时该channel越容易被选中。</p></blockquote><p>通过观察上述可视化结果，论文的作者提出了以下几个结论：</p><ol><li>低频的（index较小的）通道比高频的（index较大的）通道更容易被选择。这说明在常见的视觉任务中低频信息比高频信息携带了更多对推理有效的信息。</li><li>在亮度分量Y（luma component Y）中的通道比Cb和Cr中的通道更容易被选择。这说明YCbCr空间内的输入在推理时图像的亮度分量Y包含了更多有用信息。</li><li>根据heat map，在分类任务和分割任务上，上述两点同时成立。这证明了这些结论并非task-specific，并且可能同样适用于更多高级的计算机视觉任务。</li><li>低频的通道并不是严格地比高频的通道容易被选中。例如，在Cb和Cr上，能够观察到<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>6</mn><mo separator="true">,</mo><mn>9</mn></mrow><annotation encoding="application/x-tex">6,9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">6</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">9</span></span></span></span></span>通道被选中而<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo separator="true">,</mo><mn>4</mn><mo separator="true">,</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">3,4,5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">5</span></span></span></span></span>通道没有被选中的情况，并且在COCO数据集上 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mi>n</mi><mi>d</mi><mi>e</mi><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">index=0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">in</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0</span></span></span></span></span>对应的通道被选择的可能性低于<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mi>n</mi><mi>d</mi><mi>e</mi><mi>x</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">index=1,2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">in</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">2</span></span></span></span></span>的通道。也就是说，上述结论可能因为数据集分布的不同具有少量差异。</li></ol><p>这些结论也许说明了，CNN模型和人类的视觉一样喜欢低利用低频信息。JPEG标准在压缩图像时使用的时相似的策略。如果你想仔细研究以下JPEG标准，可以移步维基。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="什么数据输入了网络">什么数据输入了网络<a href="#什么数据输入了网络" class="hash-link" aria-label="什么数据输入了网络的直接链接" title="什么数据输入了网络的直接链接">​</a></h2><p>经过前处理得到频域特征图也并没有直接作为输入特征直接输入网络，而是与来自spatial-wise的特征图concatenate成为组合特征共同输入网络。这个过程可以表示为：</p><p><img loading="lazy" alt="image-20211023124014543" src="/assets/images/image-20211023124014543-c844095286acc53fa655fad95aa0ac0c.png" width="506" height="318" class="img_ev3q"></p><p>上图：前处理后的频域特征与直接来自图像的特征图拼接的方式。由于CNN中的卷积层对于spatial-wise的图像数据在设计上就表现出优化偏置，因此空域上的输入是必要的。上图中channel数量写为<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>64</mn></mrow><annotation encoding="application/x-tex">64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">64</span></span></span></span></span>仅为举例。根据论文描述，实际在代码中这个数字一般小等于<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>192</mn></mrow><annotation encoding="application/x-tex">192</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">192</span></span></span></span></span>。在这片论文中，作者一直将这个数字写为<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>192</mn></mrow><annotation encoding="application/x-tex">192</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">192</span></span></span></span></span>。</p><p><img loading="lazy" alt="image-20211023123829531" src="data:image/png;base64,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" width="683" height="59" class="img_ev3q"></p><p>上图：真正被输入后续网络中的数据。在concatenate操作之后，空间域的特征图和频域的特征图被拼接。经过这样的前处理，使输入的特征包含了频域信息丰富了特征表达，并且降低了占用大量带宽的空域特征图大小。</p></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/frequency-domain">frequency-domain</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/attention-mechanism">attention-mechanism</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/non-convolution">non-convolution</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.dev/neet-cv/ml.akasaki.space/blob/master/blog/[40]Learning-in-the-Frequency-Domain.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="博文分页导航"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks"><div class="pagination-nav__sublabel">较新一篇</div><div class="pagination-nav__label">Instance-sensitive Fully Convolutional Networks</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation"><div class="pagination-nav__sublabel">较旧一篇</div><div class="pagination-nav__label">CCNet - Criss-Cross Attention for Semantic Segmentation</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#概览" class="table-of-contents__link toc-highlight">概览</a></li><li><a href="#提出了频域的数据前处理过程" class="table-of-contents__link toc-highlight">提出了频域的数据前处理过程</a></li><li><a href="#提出频域的通道选择器" class="table-of-contents__link toc-highlight">提出频域的通道选择器</a></li><li><a href="#探究哪些通道被选择了" class="table-of-contents__link toc-highlight">探究哪些通道被选择了</a></li><li><a href="#什么数据输入了网络" class="table-of-contents__link toc-highlight">什么数据输入了网络</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">鲁ICP备2021025239号-2 Copyright © 2023 neet-cv.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.416d7f00.js"></script>
<script src="/assets/js/main.d42e6425.js"></script>
</body>
</html>