<!doctype html>
<html lang="zh-cn" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">You Only Look One-level Feature | 工具箱的深度学习记事簿</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://ml.akasaki.space/blog/[38]You-Only-Look-One-level-Feature"><meta data-rh="true" name="docusaurus_locale" content="zh-cn"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="zh-cn"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="You Only Look One-level Feature | 工具箱的深度学习记事簿"><meta data-rh="true" name="description" content="Qiang Chen, Yingming Wang, Tong Yang, Xiangyu Zhang, Jian Cheng, Jian Sun"><meta data-rh="true" property="og:description" content="Qiang Chen, Yingming Wang, Tong Yang, Xiangyu Zhang, Jian Cheng, Jian Sun"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2023-12-31T09:31:53.000Z"><meta data-rh="true" property="article:author" content="https://gong.host"><meta data-rh="true" property="article:tag" content="fpn,backbone,light-weight,multi-scale-learning"><link data-rh="true" rel="icon" href="/img/logo.svg"><link data-rh="true" rel="canonical" href="https://ml.akasaki.space/blog/[38]You-Only-Look-One-level-Feature"><link data-rh="true" rel="alternate" href="https://ml.akasaki.space/blog/[38]You-Only-Look-One-level-Feature" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://ml.akasaki.space/blog/[38]You-Only-Look-One-level-Feature" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="工具箱的深度学习记事簿 RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="工具箱的深度学习记事簿 Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.3231e09c.css">
<link rel="preload" href="/assets/js/runtime~main.416d7f00.js" as="script">
<link rel="preload" href="/assets/js/main.d42e6425.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">工具箱的深度学习记事簿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">魔法部日志</a><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/about">Authors &amp; About</a><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">YetAnotherAkasaki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切换浅色/暗黑模式（当前为浅色模式）" aria-label="切换浅色/暗黑模式（当前为浅色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="搜索" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="最近博文导航"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[00]unlimited-paper-works">欢迎来到魔法部日志</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[06]DeepLab-Series">DeepLab Series</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks - A Survey</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution - Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering">PointRend - Image Segmentation as Rendering</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[20]Transformer-Attention-is-all-you-need">Transformer - Attention is all you need</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features">RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet - Low-Light Enhancement Network with Global Awareness</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[26]CBAM-Convolutional-Block-Attention-Module">CBAM - Convolutional Block Attention Module</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO - Vision Outlooker for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention - Towards High-quality Pixel-wise Regression</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">CCNet - Criss-Cross Attention for Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">RepVGG - Making VGG-style ConvNets Great Again</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">PP-LCNet - A Lightweight CPU Convolutional Neural Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">Swin Transformer - Hierarchical Vision Transformer using Shifted Windows</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[46]Demystifying-Local-Vision-Transformer">Demystifying Local Vision Transformer</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation">DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations">GhostNet - More Features from Cheap Operations</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer">Kindling the Darkness - A Practical Low-light Image Enhancer</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[51]How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode">How much Position Information Do Convolutional Neural Networks Encode?</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[52]Axiomatic-Attribution-for-Deep-Networks">Axiomatic Attribution for Deep Networks</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="title_f1Hy" itemprop="headline">You Only Look One-level Feature</h1><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 17 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | 键圈躺尸砖家</small></div></div></div></div></header><div id="post-content" class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen%2C+Q" target="_blank" rel="noopener noreferrer">Qiang Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+Y" target="_blank" rel="noopener noreferrer">Yingming Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang%2C+T" target="_blank" rel="noopener noreferrer">Tong Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+X" target="_blank" rel="noopener noreferrer">Xiangyu Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng%2C+J" target="_blank" rel="noopener noreferrer">Jian Cheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun%2C+J" target="_blank" rel="noopener noreferrer">Jian Sun</a></p><blockquote><p>This paper revisits feature pyramids networks (FPN) for one-stage detectors and points out that the success of FPN is due to its divide-and-conquer solution to the optimization problem in object detection rather than multi-scale feature fusion. From the perspective of optimization, we introduce an alternative way to address the problem instead of adopting the complex feature pyramids - {\em utilizing only one-level feature for detection}. Based on the simple and efficient solution, we present You Only Look One-level Feature (YOLOF). In our method, two key components, Dilated Encoder and Uniform Matching, are proposed and bring considerable improvements. Extensive experiments on the COCO benchmark prove the effectiveness of the proposed model. Our YOLOF achieves comparable results with its feature pyramids counterpart RetinaNet while being 2.5× faster. Without transformer layers, YOLOF can match the performance of DETR in a single-level feature manner with 7× less training epochs. With an image size of 608×608, YOLOF achieves 44.3 mAP running at 60 fps on 2080Ti, which is 13% faster than YOLOv4. Code is available at <a href="https://github.com/megvii-model/YOLOF" target="_blank" rel="noopener noreferrer">this https URL</a>.</p></blockquote><p>本文简称YOLOF。截至到本文写作时，二阶段和单阶段目标检测的SOTA方法中广泛使用了多尺度特征融合的方法。FPN方法几乎已经称为了网络中理所应当的一个组件。</p><p>本文中作者重新回顾了FPN模块，并指出FPN的两个优势分别是其分治（divide-and-conquer）的解决方案、以及多尺度特征融合。本文在单阶段目标检测器上研究了FPN的这两个优势，并在RetinaNet上进行了实验，将上述两个优势解耦，分别研究其发挥的作用，并指出，FPN在多尺度特征融合上发挥的作用可能没有想象中那么大。</p><p>最后，作者提出YOLOF，这是一个不使用FPN的目标检测网络。其主要创新是：</p><ol><li>Dilated Encoder</li><li>Uniform Matching</li></ol><p>该网络在达到RetinaNet对等精度的情况下速度提升了2.5倍。</p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="讨论fpn的作用">讨论FPN的作用<a href="#讨论fpn的作用" class="hash-link" aria-label="讨论FPN的作用的直接链接" title="讨论FPN的作用的直接链接">​</a></h2><p>FPN是多尺度特征融合的经典设计，具有重大的启发意义。</p><p><img loading="lazy" alt="image-20210826204958764" src="/assets/images/image-20210826204958764-f4410325e9c689fe6899c771d030947d.png" width="656" height="455" class="img_ev3q"></p><p>上图：一个典型的FPN结构示意图。FPN结构接受来自骨干网络的多level输出作为输入，首先经过<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span>卷积的侧向连接进行通道对齐，然后经过从高级特征到低级特征不断地Upsample和按位相加操作得到融合的特征图。这样的设计在启发意义上使人认为FPN是在进行多级特征融合并由此提升性能。</p><p>同时，FPN的另一个设计动机是为了让不同尺度的目标物体分配到不同level的特征图进行预测，称之为“分而治之”的策略。这样的策略在SSD、YOLOv3等目标检测器中被使用，即使用不同level的特征图做不同尺度的目标检测。</p><p>然而，FPN的代价是在推理时存在内存复制、融合的过程，这会在原网络输出的基础上占用两倍以上的显存空间，同时会导致运算的缓慢。这导致了使用FPN的网络在资源限制下无法处理超大分辨率（例如，2K、4K分辨率或以上）的图片。</p><p>在这里，作者称FPN是一个多输入多输出（Multiple-in-Multiple-out，以下简称MiMo）的编码器（encoder），MiMo使用来自骨干网络的多级特征进行融合，然后给后续的decoder（例如各种detection head）提供多级融合的特征。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="mimosimomiso和siso">MiMo、SiMo、MiSo和SiSo<a href="#mimosimomiso和siso" class="hash-link" aria-label="MiMo、SiMo、MiSo和SiSo的直接链接" title="MiMo、SiMo、MiSo和SiSo的直接链接">​</a></h2><p>在本文中，作者将目标检测网络分为三个组成部分：</p><p><img loading="lazy" alt="image-20210826220312496" src="/assets/images/image-20210826220312496-96322e095fda749e49747f1b3f3dd0f7.png" width="1343" height="196" class="img_ev3q"></p><p>如上图，这三个组成部分分别是骨干网络（backbone，例如ResNet50），编码器（encoder，例如FPN）以及解码器（decoder，例如yolo head）。</p><p>在本文中，作者将多输入多输出（以下简称MiMo）、单输入多输出（以下简称SiMo）、多输入单输出（以下简称MiSo）和单输入单输出（以下简称SiSo）的encoder进行了检测框AP的对比。</p><p><img loading="lazy" alt="image-20210826214506814" src="/assets/images/image-20210826214506814-42b0dbae19d21a6c8d747bd097922226.png" width="730" height="356" class="img_ev3q"></p><p>上图：四种encoder的输入输出方式示意以及检测框AP。其中C3、C4以及C5分别表示骨干网络下采样到8、16、32倍的特征图；P3~P7代表最终用于检测的输出特征图。实验中使用的输入均产生自ResNet-50，并且上图中MiMo的结构和RetinaNet中使用的FPN结构相同。</p><p><img loading="lazy" alt="image-20210826230926535" src="/assets/images/image-20210826230926535-7deaccf8a024a5e230920aac33b591ed.png" width="1458" height="338" class="img_ev3q"></p><p>上图：MiMo、SiMo、MiSo、SiSo的具体结构。</p><table><thead><tr><th>英文缩写</th><th>中文释义</th><th>英文全拼</th></tr></thead><tbody><tr><td>MiMo</td><td>多输入多输出</td><td>Multiple-in-Multiple-out</td></tr><tr><td>SiMo</td><td>单输入多输出</td><td>Single-in-Multiple-out</td></tr><tr><td>MiSo</td><td>多输入单输出</td><td>Multiple-in-Single-out</td></tr><tr><td>SiSo</td><td>单输入单输出</td><td>Single-in-Single-out</td></tr></tbody></table><p>上表：四个缩写对照表。</p><p><img loading="lazy" alt="image-20210826220913659" src="/assets/images/image-20210826220913659-959151d417f56d6f5e95461a776965c4.png" width="873" height="608" class="img_ev3q"></p><p>上图：MiMo和SiSo在不同超参（channel数量等）下的性能对比。图中使用不同颜色的柱状图标记了网络的三个组成部分对性能的消耗情况，可以看出，encoder对网络的计算速度具有重大影响。<code>注：上图中最后的YOLOF和导数第二个RetinaNet是不同结构的，因此AP差异较大。</code></p><p>在本文的实验中，作者发现SiSo和MiSo的表现结果并不好，但令人惊奇的是SiMo这种并不会进行任何特征融合的结构却在AP上具有和MiMo对等的精度（相差不到1）因此作者才会提出要只使用一层特征图进行目标检测任务，即舍弃占用大量显存的FPN。</p><p><img loading="lazy" alt="image-20210826221541252" src="/assets/images/image-20210826221541252-40a7b68689259340a439ef8c9abbdc1f.png" width="577" height="286" class="img_ev3q"></p><p>上图：SiMo使用的输入输出示意。根据上述实验，仅使用C5作为输出并且不进行任何特征融合的encoder具有和MiMo对等的精度。也就是说，从backbone那里得到的C5特征图已经包含了足够完成目标检测任务的上下文信息。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="yolof的设计">YOLOF的设计<a href="#yolof的设计" class="hash-link" aria-label="YOLOF的设计的直接链接" title="YOLOF的设计的直接链接">​</a></h2><p>根据上述实验，作者提出特征图C5有能力独自承担目标检测任务。于是作者希望仅使用来自backbone的单个特征图C5+SiSo的encoder完成目标检测任务。但是，仅使用C5也导致了两个问题：</p><ol><li><p>被限制了的感受野（receptive field is limited）</p><p>与 C5 特征的感受野匹配的尺度范围是有限的，这阻碍了对不同尺度的物体的检测性能。</p></li><li><p>正样本不均衡（imbalance problem on positive anchors）</p><p>单层特征中稀疏anchor导致的正负样本的不平衡问题，老话题了。</p></li></ol><p>因此，作者使用两种方法来解决这些问题。它们是使用空洞卷积的encoder（Dilated Encoder）和统一匹配解码器（Uniform Matching）。</p><p><img loading="lazy" alt="image-20210826230512567" src="/assets/images/image-20210826230512567-c9419fb4780cd801b93ec371556d75fe.png" width="1492" height="520" class="img_ev3q"></p><p>上图：YOLOF的大致结构。可以看出，YOLOF仅使用C5进行目标检测。接下来我们聊一下使用空洞卷积的encoder（Dilated Encoder）和统一匹配解码器（Uniform Matching）的设计。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dilated-encoder-解决-limited-receptive-field">Dilated Encoder 解决 Limited receptive field<a href="#dilated-encoder-解决-limited-receptive-field" class="hash-link" aria-label="Dilated Encoder 解决 Limited receptive field的直接链接" title="Dilated Encoder 解决 Limited receptive field的直接链接">​</a></h3><p>作者想要通过标准的空洞卷积<strong>（dilated convolution）</strong>提升特征图C5的感受野，但一直下采样虽然可以让 feature 覆盖大的物体，不能很好地捕获小尺寸物体。因此，本文加入残差，将原始的 scale 范围和扩大后的 scale 范围结合起来，得到一个可以覆盖所有物体的 scale 的具有多个感受野<strong>（multiple receptive field）</strong>的输出特征，构建的Dilated Encoder结构如下：</p><p><img loading="lazy" alt="image-20210826231550035" src="/assets/images/image-20210826231550035-c204bbaed98b09b30c66daa743b02ec5.png" width="721" height="215" class="img_ev3q"></p><p>上图：本文构造的Dilated Encoder结构。其包括一个<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span>卷积+<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">3</span></span></span></span></span>卷积的projector以及四个串联的残差块。其中，四个连续的 Residual 模块中对于3x3卷积采用空洞卷积（dilated convolution）且有着不同的 dilate rate。这样以来，encoder就从C5中获得了一个具有更大感受野且不丢失小物体的特征图。<code>个人疑惑：这样做和直接从backbone取出更深的特征图的区别仅仅是空洞卷积的加入吗？</code></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="uniform-matching-解决-positive-anchors-imbalance-problem">Uniform Matching 解决 positive anchors&#x27; Imbalance problem<a href="#uniform-matching-解决-positive-anchors-imbalance-problem" class="hash-link" aria-label="Uniform Matching 解决 positive anchors&#x27; Imbalance problem的直接链接" title="Uniform Matching 解决 positive anchors&#x27; Imbalance problem的直接链接">​</a></h3><p>目标检测中的样本不平衡问题，是指对于目标检测模型，负样本的个数远多于正样本(标签为背景的检测框比标签为目标的检测框多得多)，同时，负样本中简单负样本的个数也远多于困难负样本。模型训练过程中，需要避免这种不平衡对模型性能的影响，以免模型将所有样本都判断为负样本。</p><ul><li>两阶段网络：两阶段网络中第一阶段的 RPN 与单阶段网络类似，但第二阶段 proposal 的不平衡问题得到了很大缓解，这主要是由于 RPN 处理后，proposal 中的简单负样本大大减少，从而同时减轻了正负样本和困难/简单负样本的不平衡问题。</li><li>单阶段网络：目前绝大多数单阶段网络仍然基于 anchor 。网络 anchor 的正负样本和困难/简单负样本不平衡问题十分突出。</li></ul><p>目前大家熟知的解决样本不均衡问题的解决方法有按比例随机采样（例如Faster R-CNN 中第一阶段 RPN 采用按比例随机采样）、在线难样本挖掘(OHEM, Online Hard Example Mining)、Focal Loss（这大概是最出名的专门处理不均衡的Loss设计的paper了）、IoU 平衡采样。</p><p>在anchor-based检测模型中，定义 positive anchor 的标准通常与 anchor 和 ground truth 的 IoU 有关，在 RetinaNet中，如果 anchor 与所有 ground truth 的最大 IoU 值大于 0.5，则该 anchor 是 positive anchor，此方法作者称之为 Max-IoU matching。</p><p>在 MiMo Encoder 中，anchor在 multi-level 特征图上密集的分布在整个图像上，并且ground truth会在其尺寸对应的 level 的特征图上产生 positive anchors，因此这种分治机制可以让每个尺寸的 ground truth 都能产生足够多的 positive anchors，但在 SiSo Encoder中，由于只有一个 level 特征图，且不采用分治机制（即 single out），会让 positive anchor 数量骤减。</p><p>因此，作者提出 Uniform Matching Strategy：对于每个 ground truth，采用k近邻（k nearest）的 anchors 作为 positive anchors ，这一步保证了所有的 ground truth 都能均匀的匹配到相同数量的 positive anchors 而不受 ground truth 自身 scale 的影响（正样本的平衡也会使得它们对训练的贡献相同），除此之外，在 Uniform Matching 中忽略 IoU&gt;0.7 的 negative anchors 和 IoU&lt;0.15 的 positive anchors。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="encoder-设计">Encoder 设计<a href="#encoder-设计" class="hash-link" aria-label="Encoder 设计的直接链接" title="Encoder 设计的直接链接">​</a></h2><p>参考YOLOF的结构：</p><p><img loading="lazy" alt="image-20210826230512567" src="/assets/images/image-20210826230512567-c9419fb4780cd801b93ec371556d75fe.png" width="1492" height="520" class="img_ev3q"></p><p>在backbone作者还是采用经典的ResNet和ResNeXt，选取的特征图是C5，通道数为2048且下采样率为32。</p><p><img loading="lazy" alt="image-20210826231550035" src="/assets/images/image-20210826231550035-c204bbaed98b09b30c66daa743b02ec5.png" width="721" height="215" class="img_ev3q"></p><p>上图：本文的encoder结构。encoder中第一步和FPN类似，对backbone的输出使用投影层（由1x1卷积和3x3卷积组成），得到通道数为512的特征图<code>注：FPN中使用1x1卷积投影特征图用于对齐通道</code>。接着，为了获得全尺度感受野，作者这里使用了一种残差模块，它由三个卷积组成，第一个<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span>卷积通道减少4倍，然后一个<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">3</span></span></span></span></span>膨胀卷积用于增大感受野，最后的<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span>卷积恢复通道维度，这个残差块会重复四次。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="decoder的设计">Decoder的设计<a href="#decoder的设计" class="hash-link" aria-label="Decoder的设计的直接链接" title="Decoder的设计的直接链接">​</a></h2><p>decoder和RetinaNet类似，它包含两个并行的head分支，用于目标分类和边框回归任务。作者这里主要做了两个改动：参考DETR中FFN的设计使用两个预测头以及参考AutoAssign动态分配正负样本。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="使用两个预测头">使用两个预测头<a href="#使用两个预测头" class="hash-link" aria-label="使用两个预测头的直接链接" title="使用两个预测头的直接链接">​</a></h3><p>参照DETR（End-to-End Object Detection with Transformers）中FFN的设计，使得两个head卷积层数目不同。</p><p><img loading="lazy" alt="End-to-End Object Detection with Transformers-DETR" src="/assets/images/v2-84679d261c01da7a28a67fb9509cc1ec_1440w-c115b2dedecd9783699cdb2715845848.jpg" width="1360" height="377" class="img_ev3q"></p><p>上图：DETR结构示意。其主要贡献是将目标检测任务转化为一个集合预测（set prediction）的任务，使用transformer编码-解码器结构和双边匹配的方法，由输入图像直接得到预测结果集合。和SOTA的检测方法不同，没有proposal（Faster R-CNN），没有anchor（YOLO），没有center(CenterNet)，也没有繁琐的NMS，直接预测检测框和类别，利用二分图匹配的匈牙利算法，将CNN和transformer巧妙的结合，实现目标检测的任务。</p><p>其中的预测头部（Feed-forward network，FFN）是本文参考的一个设计：</p><p><img loading="lazy" alt="image-20210827093728559" src="/assets/images/image-20210827093728559-4123a4172996e6122dae10d4c0a7ca51.png" width="1125" height="894" class="img_ev3q"></p><p>上图：FFN在DETR结构中出现的位置。在右上角的两个FFN结构不是一样的。这两个FFN通过不同的Loss分别优化，用于分别生成类别和检测框。</p><p>本文在decoder上的两个分支参考了这种设计，在回归（regression）分支中包含4个Conv-BN-ReLU操作，在分类（classification）分支中包含2个Conv-BN-ReLU操作构成两个“FFN”，使用相同的输入完成不同的生成任务。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="跳出非正即负的assign方式和监督原则">跳出非正即负的assign方式和监督原则<a href="#跳出非正即负的assign方式和监督原则" class="hash-link" aria-label="跳出非正即负的assign方式和监督原则的直接链接" title="跳出非正即负的assign方式和监督原则的直接链接">​</a></h3><p>依据AutoAssign，回归分支中的每个anchor都有一个objectness prediction，最终的分类得分由分类分支的输出乘以objectness prediction得到。</p><p>AutoAssign认为每个location众生平等（这里包括FPN各个level），每个location都有正样本属性和负样本属性。也就是说，在优化的过程中，有些样本会同时受到来自它为正样本的监督和负样本的监督。</p><p>并且，相较于大多数方法分开优化classification和regression，AutoAssign将两者进行联合，一方面可以更好地简化表示统一优化，另一方面在协助生成正样本置信度的时候可以综合考虑分类和定位的情况。即优化上regression和classification是统一的：</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>L</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><msubsup><mi>L</mi><mi>i</mi><mrow><mi>c</mi><mi>l</mi><mi>s</mi></mrow></msubsup><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>+</mo><mi>λ</mi><msubsup><mi>L</mi><mi>i</mi><mrow><mi>l</mi><mi>o</mi><mi>c</mi></mrow></msubsup><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>P</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>c</mi><mi>l</mi><mi>s</mi><mi mathvariant="normal">∣</mi><mi>θ</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mi>λ</mi><msubsup><mi>L</mi><mi>i</mi><mrow><mi>l</mi><mi>o</mi><mi>c</mi></mrow></msubsup><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mo>=</mo><mo>−</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><msub><mi>P</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>c</mi><mi>l</mi><mi>s</mi><mi mathvariant="normal">∣</mi><mi>θ</mi><mo stretchy="false">)</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>λ</mi><msubsup><mi>L</mi><mi>i</mi><mrow><mi>l</mi><mi>o</mi><mi>c</mi></mrow></msubsup><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>P</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>c</mi><mi>l</mi><mi>s</mi><mi mathvariant="normal">∣</mi><mi>θ</mi><mo stretchy="false">)</mo><msub><mi>P</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>l</mi><mi>o</mi><mi>c</mi><mi mathvariant="normal">∣</mi><mi>θ</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>P</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L_i(\theta) = L_i^{cls}(\theta) + \lambda L_i^{loc}(\theta)\\ = -\log(P_i(cls|\theta))+\lambda L_i^{loc}(\theta)\\ = -log(P_i(cls|\theta)e^{-\lambda L_i^{loc}(\theta)})\\ = -\log(P_i(cls|\theta)P_i(loc|\theta))\\ = -\log(P_i(theta))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1491em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.1491em;vertical-align:-0.25em"></span><span class="mord mathnormal">λ</span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span><span class="mord mathnormal mtight">oc</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.3669em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">c</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">s</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">))</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.1491em;vertical-align:-0.25em"></span><span class="mord mathnormal">λ</span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span><span class="mord mathnormal mtight">oc</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.3669em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.3119em;vertical-align:-0.25em"></span><span class="mord">−</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">c</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">s</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.0619em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">λ</span><span class="mord mtight"><span class="mord mathnormal mtight">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.927em"><span style="top:-2.214em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span><span class="mord mathnormal mtight">oc</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.3669em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">c</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">s</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">oc</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">))</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.3669em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mclose">))</span></span></span></span></span></div><p>这篇AutoAssign的内容是稍微有点多的，建议单独阅读一下。</p></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/fpn">fpn</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/backbone">backbone</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/light-weight">light-weight</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/multi-scale-learning">multi-scale-learning</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.dev/neet-cv/ml.akasaki.space/blob/master/blog/[38]You-Only-Look-One-level-Feature.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="博文分页导航"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation"><div class="pagination-nav__sublabel">较新一篇</div><div class="pagination-nav__label">YOLACT - Real-time Instance Segmentation</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks"><div class="pagination-nav__sublabel">较旧一篇</div><div class="pagination-nav__label">Instance-sensitive Fully Convolutional Networks</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#讨论fpn的作用" class="table-of-contents__link toc-highlight">讨论FPN的作用</a></li><li><a href="#mimosimomiso和siso" class="table-of-contents__link toc-highlight">MiMo、SiMo、MiSo和SiSo</a></li><li><a href="#yolof的设计" class="table-of-contents__link toc-highlight">YOLOF的设计</a><ul><li><a href="#dilated-encoder-解决-limited-receptive-field" class="table-of-contents__link toc-highlight">Dilated Encoder 解决 Limited receptive field</a></li><li><a href="#uniform-matching-解决-positive-anchors-imbalance-problem" class="table-of-contents__link toc-highlight">Uniform Matching 解决 positive anchors&#39; Imbalance problem</a></li></ul></li><li><a href="#encoder-设计" class="table-of-contents__link toc-highlight">Encoder 设计</a></li><li><a href="#decoder的设计" class="table-of-contents__link toc-highlight">Decoder的设计</a><ul><li><a href="#使用两个预测头" class="table-of-contents__link toc-highlight">使用两个预测头</a></li><li><a href="#跳出非正即负的assign方式和监督原则" class="table-of-contents__link toc-highlight">跳出非正即负的assign方式和监督原则</a></li></ul></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">鲁ICP备2021025239号-2 Copyright © 2023 neet-cv.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.416d7f00.js"></script>
<script src="/assets/js/main.d42e6425.js"></script>
</body>
</html>