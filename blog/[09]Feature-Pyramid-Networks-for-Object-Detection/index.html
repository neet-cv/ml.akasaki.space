<!doctype html>
<html lang="zh-cn" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">Feature Pyramid Networks for Object Detection | 工具箱的深度学习记事簿</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://ml.akasaki.space/blog/[09]Feature-Pyramid-Networks-for-Object-Detection"><meta data-rh="true" name="docusaurus_locale" content="zh-cn"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="zh-cn"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Feature Pyramid Networks for Object Detection | 工具箱的深度学习记事簿"><meta data-rh="true" name="description" content="这篇笔记的写作者是VisualDust。"><meta data-rh="true" property="og:description" content="这篇笔记的写作者是VisualDust。"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2023-12-31T09:31:53.000Z"><meta data-rh="true" property="article:author" content="https://gong.host"><meta data-rh="true" property="article:tag" content="detection,FPN,backbone,multi-scale-learning"><link data-rh="true" rel="icon" href="/img/logo.svg"><link data-rh="true" rel="canonical" href="https://ml.akasaki.space/blog/[09]Feature-Pyramid-Networks-for-Object-Detection"><link data-rh="true" rel="alternate" href="https://ml.akasaki.space/blog/[09]Feature-Pyramid-Networks-for-Object-Detection" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://ml.akasaki.space/blog/[09]Feature-Pyramid-Networks-for-Object-Detection" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="工具箱的深度学习记事簿 RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="工具箱的深度学习记事簿 Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.3231e09c.css">
<link rel="preload" href="/assets/js/runtime~main.416d7f00.js" as="script">
<link rel="preload" href="/assets/js/main.d42e6425.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">工具箱的深度学习记事簿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">魔法部日志</a><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/about">Authors &amp; About</a><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">YetAnotherAkasaki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切换浅色/暗黑模式（当前为浅色模式）" aria-label="切换浅色/暗黑模式（当前为浅色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="搜索" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="最近博文导航"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[00]unlimited-paper-works">欢迎来到魔法部日志</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[06]DeepLab-Series">DeepLab Series</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks - A Survey</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution - Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering">PointRend - Image Segmentation as Rendering</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[20]Transformer-Attention-is-all-you-need">Transformer - Attention is all you need</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features">RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet - Low-Light Enhancement Network with Global Awareness</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[26]CBAM-Convolutional-Block-Attention-Module">CBAM - Convolutional Block Attention Module</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO - Vision Outlooker for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention - Towards High-quality Pixel-wise Regression</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">CCNet - Criss-Cross Attention for Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">RepVGG - Making VGG-style ConvNets Great Again</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">PP-LCNet - A Lightweight CPU Convolutional Neural Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">Swin Transformer - Hierarchical Vision Transformer using Shifted Windows</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[46]Demystifying-Local-Vision-Transformer">Demystifying Local Vision Transformer</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation">DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations">GhostNet - More Features from Cheap Operations</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer">Kindling the Darkness - A Practical Low-light Image Enhancer</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[51]How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode">How much Position Information Do Convolutional Neural Networks Encode?</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[52]Axiomatic-Attribution-for-Deep-Networks">Axiomatic Attribution for Deep Networks</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="title_f1Hy" itemprop="headline">Feature Pyramid Networks for Object Detection</h1><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 11 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | 键圈躺尸砖家</small></div></div></div></div></header><div id="post-content" class="markdown" itemprop="articleBody"><h3 class="anchor anchorWithStickyNavbar_LWe7" id="这篇笔记的写作者是visualdust">这篇笔记的写作者是<a href="https://github.com/visualDust" target="_blank" rel="noopener noreferrer">VisualDust</a>。<a href="#这篇笔记的写作者是visualdust" class="hash-link" aria-label="这篇笔记的写作者是visualdust的直接链接" title="这篇笔记的写作者是visualdust的直接链接">​</a></h3><p>原论文<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener noreferrer">Feature Pyramid Networks for Object Detection</a>。</p><p>这篇论文就是大家熟知的FPN了。FPN是<strong>比较早期的一份工作</strong>（请注意，这篇论文只是多尺度特征融合的一种方式。不过这篇论文提出的比较早（CVPR2017），在当时看来是非常先进的），在当时具有很多亮点：FPN主要解决的是物体检测中的多尺度问题，通过简单的网络连接改变，在基本不增加原有模型计算量情况下，大幅度提升了小物体检测的性能。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="abstract摘要">Abstract（摘要）<a href="#abstract摘要" class="hash-link" aria-label="Abstract（摘要）的直接链接" title="Abstract（摘要）的直接链接">​</a></h2><blockquote><p>Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.</p></blockquote><p>这篇论文对以后的许多网络设计产生了较大的影响，推荐你阅读<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener noreferrer">原文</a>。这里只是对这篇论文的粗浅阅读笔记。</p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="介绍introduction">介绍（Introduction）<a href="#介绍introduction" class="hash-link" aria-label="介绍（Introduction）的直接链接" title="介绍（Introduction）的直接链接">​</a></h2><p>该论文提出，特征金字塔是识别系统中用于检测不同比例物体的基本组件，甚至号称手工特征设计时代的万金油：比如在OpenCV库的特征匹配Cascade分类器用于人脸识别中使用特征金字塔模型+AdaBoost提取不同尺度特征经行分类等。</p><p>原论文这样形容多尺度的好处：</p><blockquote><p>The principle advantage of featurizing each level of an image pyramid is that it produces a multi-scale feature representation in which all levels are semantically strong, including the high-resolution levels.</p></blockquote><p>在进入深度卷积网络之后，如何在卷积神经网络中更好地利用多尺度称为了一项挑战。一方面，仅使用深度卷积网络进行检测会导致小目标的漏检；另一方面，在每层不同级别的特征图上进行预测产生了多余的性能消耗，并且效果并没有想象的那么好。这篇论文就是为了解决这个问题的。</p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="相关工作related-works">相关工作（Related works）<a href="#相关工作related-works" class="hash-link" aria-label="相关工作（Related works）的直接链接" title="相关工作（Related works）的直接链接">​</a></h2><ul><li>手工设计的特征和早期神经网络（Hand-engineered features and early neural networks）</li><li>普通的深度卷积目标检测网络（Deep ConvNet object detectors）</li><li>融合了深度卷积网络的特征金字塔模型（Methods using multiple layers）</li></ul><p><img loading="lazy" alt="image-20210512231141905" src="/assets/images/image-20210512231141905-b65472fe00bc89cbbae7520f4333ccf7.png" width="1145" height="653" class="img_ev3q"></p><p>上图为原论文中出现的示意图。其中：</p><ul><li><p>(a)是手工设计特征描述时代的常见模型，即对不同尺寸的图片提取特征，以满足不同尺度目标的检测要求，提高模型性能。</p></li><li><p>(b)是普通的深度卷积网络模式，通过下采样扩大感受野，提取语义信息。</p></li><li><p>(c)是融合了深度卷积网络的特征金字塔模型。深度卷积网络在卷积过程中每层都会产生不同尺度的特征图，所以其本身就天然具有金字塔结构。截止到这篇论文的写作时间为止，有很多在COCO和ImageNet上表现优秀的网络都是pyramid representations的，即让不同层预测不同尺度的物体。但是其对小目标的检测效果仍然不够好，原因在于低尺度的特征图包含的语义信息还不够深刻（说俗话就是欠卷了），原文是这样说的：</p><blockquote><p>This in-network feature hierarchy produces feature maps of different spatial resolutions, but introduces large semantic gaps caused by different depths. The high-resolution maps have low-level features that harm their representational capacity for object recognition.</p></blockquote><p>SSD就是第一批采用这种方法的深度卷积网络之一。不过SSD为了避免使用语义信息不足的特征图，SSD并没能很好地复用已有的特征图，这让它对小目标的检测效果仍然不够好。卷积神经网络的深度往往和每一步的卷积的步长参数是一个很矛盾的东西。展开来说，网络更深的时候，就不得不通过将步长改大以平衡更深的网络带来的参数量上涨问题。但同时这导致了另外一个问题，就是步长很大的时候，甚至可以大过输入图像中一些物体的大小，使得一些目标丢失。</p></li><li><p>(d)是这篇论文要提出的FPN网络结构。该网络在设计时的一个目标就是避免(c)中出现的问题，让每一个尺度的特征图都会包含足够丰富的语义信息。</p></li></ul><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="特征金字塔网络feature-pyramid-networks">特征金字塔网络（Feature Pyramid Networks）<a href="#特征金字塔网络feature-pyramid-networks" class="hash-link" aria-label="特征金字塔网络（Feature Pyramid Networks）的直接链接" title="特征金字塔网络（Feature Pyramid Networks）的直接链接">​</a></h2><p>原论文这样描述这篇论文的目的：</p><blockquote><p>Our goal is to leverage a ConvNet’s pyramidal feature hierarchy, which has semantics from low to high levels, and build a feature pyramid with high-level semantics throughout.</p></blockquote><p>也就是说，这篇论文的目的就是利用深度卷积网络天然存在的金字塔特征层次结构：该层次结构利用自底向上的逐层的卷积获得丰富的语义信息，并在整个过程中构建具有高层语义的特征金字塔。</p><p>这篇论文的网络结构设计主要包含了两个部分：</p><ul><li>自底向上（Bottom-up pathway）</li><li>自顶向下（Top-down pathway and lateral connections）</li></ul><p>在接下来的说明中，会经常使用<code>stage</code>的概念。在这里进行提前定义说明：在这篇论文中，网络中输出的feature map大小相同的层被称为是同个stage的。不同stage会产生不同大小的特征图，这篇论文为每个stage定义一个金字塔层级（pyramid level）。</p><p><img loading="lazy" alt="image-20210513105442041" src="/assets/images/image-20210513105442041-bf83a73de8877a088888077f30e8e37f.png" width="1175" height="609" class="img_ev3q"></p><p>例如上图是VGG-16的网络结构图，图中用紫色的文字标出了5个不同的stage。当然，不挨在一起但是输出特征图大小相同的层也是属于同一个stage的。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="自底向上bottom-up-pathway">自底向上（Bottom-up pathway）<a href="#自底向上bottom-up-pathway" class="hash-link" aria-label="自底向上（Bottom-up pathway）的直接链接" title="自底向上（Bottom-up pathway）的直接链接">​</a></h3><p>自底向上就是普通深度卷积网络前向传播的过程。对于提及的金字塔（pyramid）特征，这篇论文选择每个stage最后一层的输出作为一个特征图，这个选择听上去很合理，因为越上层（每个stage的最后一个层）的特征就包含更多的语义信息。</p><p><img loading="lazy" alt="image-20210513105825996" src="/assets/images/image-20210513105825996-5e0eb11494ad89300caf9a419b3dba24.png" width="733" height="278" class="img_ev3q"></p><p>例如，在上图中，这篇论文的方法选择右侧的特征图输出作为一个stage的特征图输出而不是左侧的。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="自顶向下top-down-pathway-and-lateral-connections">自顶向下（Top-down pathway and lateral connections）<a href="#自顶向下top-down-pathway-and-lateral-connections" class="hash-link" aria-label="自顶向下（Top-down pathway and lateral connections）的直接链接" title="自顶向下（Top-down pathway and lateral connections）的直接链接">​</a></h3><p>自顶向下的结构在直观感受上是和深度卷积网络下采样的过程相反的：它将高层stage输出的空间信息很粗劣（毕竟之前一直在下采样...）但是语义信息很丰富的特征图进行不断上采样，上采样到和其对应的stage的上一个stage的特征图大小，再通过图中这种侧边的连接（lateral connections），使用低一级stage的特征图来增强它。</p><p><img loading="lazy" alt="image-20210513110511093" src="/assets/images/image-20210513110511093-4117637496c0968d7839c6776dd6b66e.png" width="929" height="496" class="img_ev3q"></p><p>上图是论文中侧边连接（lateral connections）的方式。每个侧边连接将同个stage的自底向上的特征图和自顶向下的特征图相加。注意，该论文中特指了每两个stage之间的空间分辨率之差是2倍，也就是下采样率是2倍。所以上图的示意中靠近顶层的特征图经过2倍上采样后使用侧边连接和上一级特征图相加组成增强后的新特征；技术上，在侧边连接的过程中，这篇论文的方法使用<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span>卷积的方式使需要相加的特征图通道数保持一致。</p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="应用applications">应用（Applications）<a href="#应用applications" class="hash-link" aria-label="应用（Applications）的直接链接" title="应用（Applications）的直接链接">​</a></h2><p>有空就写。</p></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/detection">detection</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/fpn">FPN</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/backbone">backbone</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/multi-scale-learning">multi-scale-learning</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.dev/neet-cv/ml.akasaki.space/blob/master/blog/[09]Feature-Pyramid-Networks-for-Object-Detection.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="博文分页导航"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/[08]Dynamic-Neural-Networks-A-Survey"><div class="pagination-nav__sublabel">较新一篇</div><div class="pagination-nav__label">Dynamic Neural Networks - A Survey</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/[10]Overview-Of-Semantic-Segmentation"><div class="pagination-nav__sublabel">较旧一篇</div><div class="pagination-nav__label">A Review on Deep Learning Techniques Applied to Semantic Segmentation</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#这篇笔记的写作者是visualdust" class="table-of-contents__link toc-highlight">这篇笔记的写作者是VisualDust。</a></li><li><a href="#abstract摘要" class="table-of-contents__link toc-highlight">Abstract（摘要）</a></li><li><a href="#介绍introduction" class="table-of-contents__link toc-highlight">介绍（Introduction）</a></li><li><a href="#相关工作related-works" class="table-of-contents__link toc-highlight">相关工作（Related works）</a></li><li><a href="#特征金字塔网络feature-pyramid-networks" class="table-of-contents__link toc-highlight">特征金字塔网络（Feature Pyramid Networks）</a><ul><li><a href="#自底向上bottom-up-pathway" class="table-of-contents__link toc-highlight">自底向上（Bottom-up pathway）</a></li><li><a href="#自顶向下top-down-pathway-and-lateral-connections" class="table-of-contents__link toc-highlight">自顶向下（Top-down pathway and lateral connections）</a></li></ul></li><li><a href="#应用applications" class="table-of-contents__link toc-highlight">应用（Applications）</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">鲁ICP备2021025239号-2 Copyright © 2023 neet-cv.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.416d7f00.js"></script>
<script src="/assets/js/main.d42e6425.js"></script>
</body>
</html>