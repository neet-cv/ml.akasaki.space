<!doctype html>
<html lang="zh-cn" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">7 篇博文 含有标签「low-light」 | 工具箱的深度学习记事簿</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://ml.akasaki.space/blog/tags/low-light"><meta data-rh="true" name="docusaurus_locale" content="zh-cn"><meta data-rh="true" name="docsearch:language" content="zh-cn"><meta data-rh="true" property="og:title" content="7 篇博文 含有标签「low-light」 | 工具箱的深度学习记事簿"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/img/logo.svg"><link data-rh="true" rel="canonical" href="https://ml.akasaki.space/blog/tags/low-light"><link data-rh="true" rel="alternate" href="https://ml.akasaki.space/blog/tags/low-light" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://ml.akasaki.space/blog/tags/low-light" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="工具箱的深度学习记事簿 RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="工具箱的深度学习记事簿 Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.3231e09c.css">
<link rel="preload" href="/assets/js/runtime~main.416d7f00.js" as="script">
<link rel="preload" href="/assets/js/main.d42e6425.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">工具箱的深度学习记事簿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">魔法部日志</a><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/about">Authors &amp; About</a><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">YetAnotherAkasaki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切换浅色/暗黑模式（当前为浅色模式）" aria-label="切换浅色/暗黑模式（当前为浅色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="搜索" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="最近博文导航"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[00]unlimited-paper-works">欢迎来到魔法部日志</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[06]DeepLab-Series">DeepLab Series</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks - A Survey</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution - Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering">PointRend - Image Segmentation as Rendering</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[20]Transformer-Attention-is-all-you-need">Transformer - Attention is all you need</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features">RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet - Low-Light Enhancement Network with Global Awareness</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[26]CBAM-Convolutional-Block-Attention-Module">CBAM - Convolutional Block Attention Module</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO - Vision Outlooker for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention - Towards High-quality Pixel-wise Regression</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">CCNet - Criss-Cross Attention for Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">RepVGG - Making VGG-style ConvNets Great Again</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">PP-LCNet - A Lightweight CPU Convolutional Neural Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">Swin Transformer - Hierarchical Vision Transformer using Shifted Windows</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[46]Demystifying-Local-Vision-Transformer">Demystifying Local Vision Transformer</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation">DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations">GhostNet - More Features from Cheap Operations</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer">Kindling the Darkness - A Practical Low-light Image Enhancer</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[51]How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode">How much Position Information Do Convolutional Neural Networks Encode?</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[52]Axiomatic-Attribution-for-Deep-Networks">Axiomatic Attribution for Deep Networks</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>7 篇博文 含有标签「low-light」</h1><a href="/blog/tags">查看所有标签</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 16 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>这是一篇讲低光照人脸检测的论文。<a href="https://arxiv.org/pdf/2104.01984.pdf" target="_blank" rel="noopener noreferrer">原论文（HLA-Face Joint High-Low Adaptation for Low Light Face Detection）</a>。</p><ul><li>充分利用现有的正常光数据，并探索如何将面部探测器从正常光线调整到低光。这项任务的挑战是，正常和低光之间的差距对于像素级和物体级别来说太大而复杂。因此，大多数现有的lowlighenhance和适应方法不达到所需的performance。</li><li>本文是DARK FACE为基准，针对现有的正常照度图像，将图像调整成低照度图像，不需要标签</li><li>一个是像素级外观的差距，例如不足，照明，相机噪声和颜色偏置。另一个是正常和低光场景之间的物体级语义差异，包括但不限于路灯的存在，车辆前灯和广告板。传统的低光增强方法<!-- -->[5,6]<!-- -->设计用于提高视觉质量，因此不能填充语义差距，</li><li>通过使低光图像亮起并扭曲正常光图像，我们构建位于正常和低光之间的中间状态。</li></ul><p>摘要:</p><blockquote><p>Face detection in low light scenarios is challenging but vital to many practical applications, e.g., surveillance video, autonomous driving at night. Most existing face detectors heavily rely on extensive annotations, while col- lecting data is time-consuming and laborious. To reduce the burden of building new datasets for low light condi- tions, we make full use of existing normal light data and explore how to adapt face detectors from normal light to low light. The challenge of this task is that the gap between normal and low light is too huge and complex for both pixel-level and object-level. Therefore, most existing low- light enhancement and adaptation methods do not achieve desirable performance. To address the issue, we propose a joint High-Low Adaptation (HLA) framework. Through a bidirectional low-level adaptation and multi-task high- level adaptation scheme, our HLA-Face outperforms state- of-the-art methods even without using dark face labels for training. Our project is publicly available at: <!-- -->[https: //daooshee.github.io/HLA-Face-Website/]<!-- -->(https: //daooshee.github.io/HLA-Face-Website/)</p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/detection">detection</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/low-light">low-light</a></li></ul></div><div class="col text--right col--3"><a aria-label="阅读 HLA-Face Joint High-Low Adaptation for Low Light Face Detection 的全文" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection"><b>阅读更多</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 21 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>论文名称: <a href="https://arxiv.org/abs/1808.04560" target="_blank" rel="noopener noreferrer">Deep Retinex Decomposition for Low-Light Enhancement</a></p><p>论文作者: Chen Wei, Wenjing Wang, Wenhan Yang, Jiaying Liu</p><p>Code: <a href="https://github.com/weichen582/RetinexNet" target="_blank" rel="noopener noreferrer">https://github.com/weichen582/RetinexNet</a></p></blockquote><p>这是一篇讲解使用卷积神经网络进行低照度增强的论文。</p><ul><li>采用了分解网络和增强网络，使用Retinex理论构建分解网络，分解后再进行增强。</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="abstract-摘要">Abstract (摘要)<a href="#abstract-摘要" class="hash-link" aria-label="Abstract (摘要)的直接链接" title="Abstract (摘要)的直接链接">​</a></h2><blockquote><p>Retinex model is an effective tool for low-light image enhancement. It assumes that observed images can be decomposed into the reflectance and illumination. Most existing Retinex-based methods have carefully designed hand-crafted constraints and parameters for this highly ill-posed decomposition, which may be limited by model capacity when applied in various scenes. In this paper, we collect a LOw-Light dataset (LOL) containing low/normal-light image pairs and propose a deepRetinex-Netlearned on this dataset, including a Decom-Net for decomposition and an Enhance-Net for illumination adjusment. In the training process for Decom-Net, there is no ground truth of decomposed reflectance and illumination. The network is learned with only key constraints including the consistent reflectance shared by paired low/normal-light images, and the smoothness of illumination. Based on the decomposition, subsequent lightness enhancement is conducted on illumination by an enhancement network called Enhance-Net, and for joint denoising there is a denoising operation on reflectance. The Retinex-Net is end-to-end trainable, so that the learned decomposition is by nature good for lightness adjustment. Extensive experiments demonstrate that our method not only achieves visually pleasing quality for low-light enhancement but also provides a good representation of image decomposition.</p></blockquote><p>Retinex模型是弱光图像增强的有效工具。它假设观察到的图像<strong>可以分解为反射率和照度</strong>。大多数现有的基于视网膜的方法都为这种高度不适定的分解精心设计了手工制作的约束和参数，当应用于各种场景时，这可能会受到模型容量的限制。在本文中，我们收集了一个包含弱光/正常光图像对的弱光数据集，并在此数据集上提出了一个Deeprinex Netlearn，包括一个用于分解的Decom-Net和一个用于光照调整的增强-Net。在Decom-Net的训练过程中，不存在分解反射率和光照的基本事实。该网络仅在关键约束条件下学习，包括成对的弱光/正常光图像共享的一致反射率以及照明的平滑度。在分解的基础上，通过增强网络对光照进行后续的亮度增强，对于联合去噪，对反射率进行去噪操作。RetinexNet是端到端训练的，因此学习的分解本质上有利于亮度调整。大量实验表明，我们的方法不仅在弱光增强方面获得了视觉上令人满意的质量，而且提供了图像分解的良好表示。</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/low-light">low-light</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/backbone">backbone</a></li></ul></div><div class="col text--right col--3"><a aria-label="阅读 Deep Retinex Decomposition for Low-Light Enhancement 的全文" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement"><b>阅读更多</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 15 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>论文名称: <a href="https://arxiv.org/abs/1711.02488" target="_blank" rel="noopener noreferrer">MSR-net:Low-light Image Enhancement Using Deep Convolutional Network</a></p><p>论文作者: Liang Shen, Zihan Y ue, Fan Feng, Quan Chen, Shihao Liu, Jie Ma</p><p>Code: None</p></blockquote><p>这是一篇讲解基于Retinex理论使用卷积神经网络进行低照度增强的论文。</p><ul><li>基于MSR传统理论构造卷积神经网络模型</li><li>直接学习暗图像和亮图像之间的端到端映射</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="abstract-摘要">Abstract (摘要)<a href="#abstract-摘要" class="hash-link" aria-label="Abstract (摘要)的直接链接" title="Abstract (摘要)的直接链接">​</a></h2><blockquote><p>Images captured in low-light conditions usually suffer from very low contrast, which increases the difficulty of sub-sequent computer vision tasks in a great extent. In this paper, a low-light image enhancement model based on convolutional neural network and Retinex theory is proposed. Firstly, we show that multi-scale Retinex is equivalent to a feedforward convolutional neural network with different Gaussian convolution kernels. Motivated by this fact, we consider a Convolutional Neural Network(MSR-net) that directly learns an end-to-end mapping between dark and bright images. Different fundamentally from existing approaches, low-light image enhancement in this paper is regarded as a machine learning problem. In this model, most of the parameters are optimized by back-propagation, while the parameters of traditional models depend on the artificial setting. Experiments on a number of challenging images reveal the advantages of our method in comparison with other state-of-the-art methods from the qualitative and quantitative perspective.</p></blockquote><p>本文提出了一种基于卷积神经网络和视网膜理论(Retinex Theory)的低照度图像增强模型。证明了多尺度视网膜等价于一个具有不同高斯卷积核的前馈卷积神经网络。考虑一种卷积神经网络(MSR网络)，它<strong>直接学习暗图像和亮图像之间的端到端映射</strong>。</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/low-light">low-light</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/multi-scale-learning">multi-scale-learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="阅读 MSR-net - Low-light Image Enhancement Using Deep Convolutional Network 的全文" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network"><b>阅读更多</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 11 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>论文名称: <a href="https://ieeexplore.ieee.org/abstract/document/8305143/" target="_blank" rel="noopener noreferrer">LLCNN: A convolutional neural network for low-light image enhancement</a></p><p>论文作者: Li Tao, Chuang Zhu, Guoqing Xiang, Yuan Li, Huizhu Jia, Xiaodong Xie</p><p>Code: <a href="https://github.com/BestJuly/LLCNN" target="_blank" rel="noopener noreferrer">https://github.com/BestJuly/LLCNN</a></p></blockquote><h3 class="anchor anchorWithStickyNavbar_LWe7" id="这篇笔记的写作者是pommespeter">这篇笔记的写作者是<a href="https://github.com/PommesPeter" target="_blank" rel="noopener noreferrer">PommesPeter</a>。<a href="#这篇笔记的写作者是pommespeter" class="hash-link" aria-label="这篇笔记的写作者是pommespeter的直接链接" title="这篇笔记的写作者是pommespeter的直接链接">​</a></h3><p>这是一篇讲解使用卷积神经网络进行低照度增强的论文。</p><ul><li>本文使用卷积神经网络进行低照度增强</li><li>使用SSIM损失更好地评价图像好坏和梯度收敛</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="abstract-摘要">Abstract (摘要)<a href="#abstract-摘要" class="hash-link" aria-label="Abstract (摘要)的直接链接" title="Abstract (摘要)的直接链接">​</a></h2><blockquote><p>In this paper, we propose a CNN based method to perform low-light image enhancement. We design a special  module to utilize multiscale feature maps, which can avoid  gradient vanishing problem as well. In order to preserve image textures as much as possible, we use SSIM loss to train our model. The contrast of low-light images can be adaptively enhanced using our method. Results demonstrate that our CNN based method  outperforms other contrast enhancement methods. </p></blockquote><p>本文提出了一种基于CNN的低照度图像增强方法。我们设计了一个特殊的模块来<strong>利用多尺度特征映射</strong>，这样可以避免梯度消失的问题。<strong>为了尽可能地保留图像纹理，我们使用SSIM损失来训练我们的模型</strong>。使用我们的方法可以<strong>自适应地增强弱光图像的对比度</strong>。</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/low-light">low-light</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/multi-scale-learning">multi-scale-learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="阅读 LLCNN - A convolutional neural network for low-light image enhancement 的全文" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement"><b>阅读更多</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 15 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/RuoMengAwA" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220116171846.jpg" alt="RuoMengAwA"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/RuoMengAwA" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">RuoMengAwA</span></a></div><small class="avatar__subtitle" itemprop="description">一个喜欢摸鱼的菜狗，目前主要做低照度增强方向的研究</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>该论文提出时间2018.7.20</p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="大纲">大纲<a href="#大纲" class="hash-link" aria-label="大纲的直接链接" title="大纲的直接链接">​</a></h2><blockquote><p>对于Retinex方法，其为一种有效的低照度增强方法，它将观察到的图像分解为反射率和照度</p></blockquote><p>大多数现有的Retinex方法都需要花费大量精力去设置分解的参数，以达到较好的效果，但是这样在实际场景中效果较差，而在这篇论文中，作者收集了一个低照度与正常光对比的低光数据集并基于该数据集的学习提出了一个Deep Retinex-net </p><blockquote><p>Deep Retinex-net其中包括了一个 Decom-Net 用于分解 以及一个 Enhance-Net用于照度调节</p></blockquote><blockquote><p>Decom-Net：（分解）在训练过程中<strong>不考虑</strong>分解后反射率和光照的基本事实，而是只学习两个关键的约束条件，低照度到正常图像共享的<strong>一致反射率</strong>以及照明的<strong>平滑度</strong></p></blockquote><blockquote><p>Enhance-Net：（增强）基于分解的基础，进行亮度增强</p></blockquote><p>对于联合去噪，存在对于反射率的去噪操作，而在Retinex-net中是端到端可训练的，因此，对于分解的学习过程有助于亮度调整。</p><p>经过大量实验表明，作者的方法在视觉上的弱光增强获得了令人满意的效果，并且拥有图像分解的良好表现</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/low-light">low-light</a></li></ul></div><div class="col text--right col--3"><a aria-label="阅读 Deep Retinex Decomposition for Low-Light Enhancement 的全文" href="/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement"><b>阅读更多</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations">GhostNet - More Features from Cheap Operations</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 19 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/RuoMengAwA" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220116171846.jpg" alt="RuoMengAwA"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/RuoMengAwA" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">RuoMengAwA</span></a></div><small class="avatar__subtitle" itemprop="description">一个喜欢摸鱼的菜狗，目前主要做低照度增强方向的研究</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p><em>鬼网！</em></p><p><img loading="lazy" src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212350.png" alt="image-20210510203042875" class="img_ev3q"></p></blockquote><hr><h1>GhostNet产生原因</h1><blockquote><p><em>mobileNet或者是shuffleNet提出了使用depthwise或者是shuffle等操作，但是引入的1x1卷积依然会产生一定的计算量</em></p><p>为什么1x1卷积依然会产生较大的计算量?看卷积计算量的计算公式$n ∗ h ∗ w ∗ c ∗ k ∗ k $,可以发现，由于c和n都是比较大的，所以会导致这个计算量也是比较大的（后文具体结构复现时还会解释）</p><p>所以，我们如何在这个基础上再减少参数，优化网络速度呢，作者从一个独特的角度，观察了ResNet50第一个残差块输出的特征图，发现有许多输出特征很相似，基本只要进行简单的线性变换就能得到，而不需要进行复杂的非线性变换得到。</p><p>如图：</p><p><img loading="lazy" src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212347.png" alt="image-20210510184255116" class="img_ev3q"></p><p>以上图片中同色图片可以使用cheap operations进行生成</p><p>所以可以先通过一个非线性变化得到其中一个特征图，针对这个特征图做线性变化，得到原特征图的幽灵特征图。</p><p><em>ps:这里说的非线性卷积操作是卷积-批归一化-非线性激活全套组合，而所谓的线性变换或者廉价操作均指普通卷积，不含批归一化和非线性激活</em></p></blockquote><p>​		 所以，总结其<strong>核心思想</strong>就是：设计一种分阶段的卷积计算模块，在少量的非线性的卷积得到的特征图基础上，再进行一次线性卷积，从而获取更多的特征图，而新得到的特征图，就被叫做之前特征图的‘ghost’，以此来实现消除冗余特征（也可以说是不避免冗余的特征映射，而是使用一种更低成本效益的方式接受它），使得在保持相似的识别性能的同时，降低通用卷积层的计算代价，以获取更加轻量的模型（非线性的操作是 <em>昂贵的</em>，线性操作是 <em>廉价的</em>）（这操作鬼想得到。。。）</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/low-light">low-light</a></li></ul></div><div class="col text--right col--3"><a aria-label="阅读 GhostNet - More Features from Cheap Operations 的全文" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations"><b>阅读更多</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer">Kindling the Darkness - A Practical Low-light Image Enhancer</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 12 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/RuoMengAwA" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220116171846.jpg" alt="RuoMengAwA"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/RuoMengAwA" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">RuoMengAwA</span></a></div><small class="avatar__subtitle" itemprop="description">一个喜欢摸鱼的菜狗，目前主要做低照度增强方向的研究</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="研究背景">研究背景：<a href="#研究背景" class="hash-link" aria-label="研究背景：的直接链接" title="研究背景：的直接链接">​</a></h2><p>对于一张低光图像，不仅是暗，而且也会伴随着噪声和颜色失真等多方面的图像功能退化，所以仅仅提高亮度将无可避免的提高人工产生的影响，必然会放大隐藏的伪影</p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="特点">特点：<a href="#特点" class="hash-link" aria-label="特点：的直接链接" title="特点：的直接链接">​</a></h2><p>还是从retinex理论中得到的启发，继而将弱光图像分解为光照（<strong>illumination</strong>）和 反射率（<strong>reflectance</strong>）；前者负责亮度调整，后者用于去除降质（噪声，颜色失真）。这样图像分解的好处是让每一个模块可以更好地被正规化/学习</p><p>而对于输入图像，该网络只需要使用两张不同曝光条件下的图像（即使他们是两张弱光图像也可以），而不是弱光图像和真实图像（这样的好处是，很难定义多亮的图像算是真实图像）</p><p>对于严重的视觉缺陷图片也依旧拥有很强的鲁棒性</p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="效果">效果：<a href="#效果" class="hash-link" aria-label="效果：的直接链接" title="效果：的直接链接">​</a></h2><p>模型在2080Ti下的训练速度为，处理一张VGA分辨率图片花费的时间不到50ms</p><p>用户可以自由的调节光照水平（暂时没看到在哪体现）</p><p>具体效果展示（实机测试）：</p><p>不同噪度：</p><blockquote><p>高光图像和低光图像对照（不同的）</p></blockquote><p><img loading="lazy" src="https://xiaomai-aliyunoss.oss-cn-shenzhen.aliyuncs.com/img/20220117212643.png" alt="image-20210802180913986" class="img_ev3q"></p><p>可以得出，KinD在多条件下，效果暂时都优于其他低照度优化算法（最主要的是效果真实，相较于其余算法，失真的情况会大大减少（不过现在还有一个KinD++））</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/low-light">low-light</a></li></ul></div><div class="col text--right col--3"><a aria-label="阅读 Kindling the Darkness - A Practical Low-light Image Enhancer 的全文" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer"><b>阅读更多</b></a></div></footer></article><nav class="pagination-nav" aria-label="博文列表分页导航"></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">鲁ICP备2021025239号-2 Copyright © 2023 neet-cv.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.416d7f00.js"></script>
<script src="/assets/js/main.d42e6425.js"></script>
</body>
</html>