<!doctype html>
<html lang="zh-cn" dir="ltr" class="blog-wrapper blog-list-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">Blog | 工具箱的深度学习记事簿</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://ml.akasaki.space/blog"><meta data-rh="true" name="docusaurus_locale" content="zh-cn"><meta data-rh="true" name="docsearch:language" content="zh-cn"><meta data-rh="true" property="og:title" content="Blog | 工具箱的深度学习记事簿"><meta data-rh="true" name="description" content="Blog"><meta data-rh="true" property="og:description" content="Blog"><meta data-rh="true" name="docusaurus_tag" content="blog_posts_list"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/img/logo.svg"><link data-rh="true" rel="canonical" href="https://ml.akasaki.space/blog"><link data-rh="true" rel="alternate" href="https://ml.akasaki.space/blog" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://ml.akasaki.space/blog" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="工具箱的深度学习记事簿 RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="工具箱的深度学习记事簿 Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.3231e09c.css">
<link rel="preload" href="/assets/js/runtime~main.416d7f00.js" as="script">
<link rel="preload" href="/assets/js/main.d42e6425.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">工具箱的深度学习记事簿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">魔法部日志</a><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/about">Authors &amp; About</a><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">YetAnotherAkasaki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切换浅色/暗黑模式（当前为浅色模式）" aria-label="切换浅色/暗黑模式（当前为浅色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="搜索" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="最近博文导航"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[00]unlimited-paper-works">欢迎来到魔法部日志</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[06]DeepLab-Series">DeepLab Series</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks - A Survey</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2 - Inverted Residuals and Linear Bottlenecks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN - Fast Semantic Segmentation Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU - Improving Object-Centric Image Segmentation Evaluation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution - Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[19]PointRend-Image-Segmentation-as-Rendering">PointRend - Image Segmentation as Rendering</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[20]Transformer-Attention-is-all-you-need">Transformer - Attention is all you need</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[21]RefineMask-Towards-High-Quality-Instance-Segmentationwith-Fine-Grained_Features">RefineMask - Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet - Low-Light Enhancement Network with Global Awareness</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet - Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[26]CBAM-Convolutional-Block-Attention-Module">CBAM - Convolutional Block Attention Module</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net - Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN - A convolutional neural network for low-light image enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO - Vision Outlooker for Visual Recognition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention - Towards High-quality Pixel-wise Regression</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM - A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[36]SOLO-Segmenting-Objects-by-Locations">SOLO - Segmenting Objects by Locations</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT - Real-time Instance Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">CCNet - Criss-Cross Attention for Semantic Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">RepVGG - Making VGG-style ConvNets Great Again</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">PP-LCNet - A Lightweight CPU Convolutional Neural Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">Swin Transformer - Hierarchical Vision Transformer using Shifted Windows</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[46]Demystifying-Local-Vision-Transformer">Demystifying Local Vision Transformer</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[47]Discrete-Cosine-Transform-Mask-Representation-for-Instance-Segmentation">DCT-Mask - Discrete Cosine Transform Mask Representation for Instance Segmentation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[48]Deep-Retinex-Decomposition-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[49]GhostNet-More-Features-from-Cheap-Operations">GhostNet - More Features from Cheap Operations</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[50]Kindling-the-Darkness-A-Practical-Low-light-Image-Enhancer">Kindling the Darkness - A Practical Low-light Image Enhancer</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[51]How-much-Position-Information-Do-Convolutional-Neural-Networks-Encode">How much Position Information Do Convolutional Neural Networks Encode?</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/[52]Axiomatic-Attribution-for-Deep-Networks">Axiomatic Attribution for Deep Networks</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/[00]unlimited-paper-works">欢迎来到魔法部日志</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 40 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | 键圈躺尸砖家</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>如果你看到了这里，说明你已经准备好开始探求这个领域事物的规律以及这些规律的本源了。作为一个本科生，最适合你入坑的就是开始习惯性阅读领域内论文。大约在我的大二的下半学期，我和我的朋友们开始共同阅读论文并写下笔记。这些笔记粗浅、幼稚，甚至会出现一些理解上的错误——万事开头难。但是我们还是想把这些笔记整理起来——这便是魔法部日志的开始。在我新建文件夹的时候，魔法部日志的文件夹名称是“unlimited paper works”，在成为理性的怀疑者之前，应该先掌握这个科研领域。我们做好了长期投入的准备，并希望把简单的事情做到出人意料得精彩。</p><p>加入魔法部日志也不是什么难事，你只需要热身一下，读完下面的一篇引导，就可以开始了(以下内容已通过语法检查工具<a href="https://github.com/PaperCube" target="_blank" rel="noopener noreferrer">PaperCube</a>的检查)。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-read-and-comprehend-scientific-research-articles">How to Read and Comprehend Scientific Research Articles<a href="#how-to-read-and-comprehend-scientific-research-articles" class="hash-link" aria-label="How to Read and Comprehend Scientific Research Articles的直接链接" title="How to Read and Comprehend Scientific Research Articles的直接链接">​</a></h2><p>Scientific articles are how scholars and researchers communicate with each other. Reading scientific articles helps you to participate in your comprehension by wondering how the researchers explain their ideas. Books, websites, papers, scientific magazines are general places to start with.</p><p>This tutorial will discuss:</p><ul><li>How to read a scientific article</li><li>How to find the main points of an article</li><li>How to take effective notes</li></ul></div><footer class="row docusaurus-mt-lg"><div class="col text--right"><a aria-label="阅读 欢迎来到魔法部日志 的全文" href="/blog/[00]unlimited-paper-works"><b>阅读更多</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder - Classification, Regression and GANs</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 15 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | 键圈躺尸砖家</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>这是一篇讲各种各样解码器的论文。<a href="https://arxiv.org/pdf/1707.05847.pdf" target="_blank" rel="noopener noreferrer">原论文（The Devil is in the Decoder: Classification, Regression and GANs）</a>。</p><p>由于“解码器（decoder，有些时候也被称为feature extractor）”的概念与像素级的分类、回归等问题多多少少都有瓜葛。以下是decoder被应用于像素级的任务：</p><ul><li>分类：语义分割、边缘检测。</li><li>回归：人体关键点检测、深度预测、着色、超分辨。</li><li>合成：利用生成对抗网络生成图像等。</li></ul><p>所以decoder是稠密预测（Dence prediction，像素级别的很多问题都可以叫做稠密的）问题的关键。</p><h1>Abstract（摘要）</h1><blockquote><p>Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.</p></blockquote><p>我看了这篇综述受益匪浅，如果有时间的话请阅读<a href="/papers/The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.pdf">原作</a>。本文只是对原作阅读的粗浅笔记。</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/survey">survey</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/decoder">decoder</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/segmentation">segmentation</a></li></ul></div><div class="col text--right col--3"><a aria-label="阅读 The Devil is in the Decoder - Classification, Regression and GANs 的全文" href="/blog/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs"><b>阅读更多</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 102 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/AndSonder" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/AndSonder.png" alt="Sonder"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/AndSonder" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sonder</span></a></div><small class="avatar__subtitle" itemprop="description">life is but a span, I use python</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>这是一篇神经对抗的综述文章，非常非常非常详细的介绍了当前神经对抗攻击的发展情况和已有的攻击和防御算法。原论文：<a href="https://arxiv.org/pdf/1801.00553.pdf" target="_blank" rel="noopener noreferrer">Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey</a></p><blockquote><p>Deep learning is at the heart of the current rise of machine learning and artificial intelligence. In the field of Computer Vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has lead to a large influx of contributions in this direction. This article presents the first comprehensive survey on adversarial attacks on deep learning in Computer Vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, we draw on the literature to provide a broader outlook of the research direction.</p></blockquote><p>本文主要对文章进行翻译，还加入了个人对一些算法的理解与解释。这篇文章我大概看了一个星期。真的是一篇非常不错的综述论文。</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/adversarial-attacks">adversarial attacks</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/survey">survey</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/robustness">robustness</a></li></ul></div><div class="col text--right col--3"><a aria-label="阅读 Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey 的全文" href="/blog/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey"><b>阅读更多</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 6 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/zeroRains.png" alt="Zerorains"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Zerorains</span></a></div><small class="avatar__subtitle" itemprop="description">life is but a span, I use python</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>原论文：<a href="https://arxiv.org/pdf/2104.03778.pdf" target="_blank" rel="noopener noreferrer">Progressive Semantic Segmentation</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="问题描述">问题描述<a href="#问题描述" class="hash-link" aria-label="问题描述的直接链接" title="问题描述的直接链接">​</a></h2><p>当对大型图片进行语义分割时，可能会导致显存炸掉。收到内存限制，可以选择下采样，或将图像划分为局部块。但前者会丢失细节，后者会却反全局视图。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="后处理改善分割细节">后处理改善分割细节<a href="#后处理改善分割细节" class="hash-link" aria-label="后处理改善分割细节的直接链接" title="后处理改善分割细节的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="经典方法">经典方法<a href="#经典方法" class="hash-link" aria-label="经典方法的直接链接" title="经典方法的直接链接">​</a></h3><p>条件随机场(CRF),引导滤波器（GF），两个速度慢，改进是渐进的。</p><p>深度学习的引导过滤器(DGF)可以提高推理速度</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/refinement">refinement</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/multi-scale-learning">multi-scale-learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="阅读 Progressive Semantic Segmentation 的全文" href="/blog/[03]Progressive-Semantic-Segmentation"><b>阅读更多</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 22 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | 键圈躺尸砖家</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>这是一篇关于数据依赖型解码器的理论和测试工作的论文。原论文是<a href="https://arxiv.org/pdf/1903.02120.pdf" target="_blank" rel="noopener noreferrer">Decoders Matter for Semantic Segmentation : Data-Dependent Decoding Enables Flexible Feature Aggregation</a>。</p><p>近年来，常见的语义分割方法利用编码器-解码器结构进行逐像素的预测任务。在这些解码器每一层的最后通常是一层双线性上采样的过程，用于将像素恢复至原有像素大小。本论文的研究表明，这种与数据无关的双线性上采样方法可能会导致结果并不完美。</p><p>所以，本论文提出了一种依赖于输入数据的上采样取代双线性上采样，称为“DUpsampling”。这个新的方法利用在语义分段标签中的空间冗余，能够从低分辨率的CNN输出中恢复分辨率并实现逐像素预测。该方法在分辨率相对较低的输入上能获得更加精确的分割效果，并且显著降低了计算的复杂度。也就是说：</p><ul><li>这种新的上采样层重建能力非常强</li><li>这种方法对任何CNN编码器的组合和使用表现出很好的兼容性</li></ul><p>本论文还通过实验标明了，DUpsampling性能优越，并且无需任何后处理。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="abstract摘要">Abstract（摘要）<a href="#abstract摘要" class="hash-link" aria-label="Abstract（摘要）的直接链接" title="Abstract（摘要）的直接链接">​</a></h2><blockquote><p>Recent semantic segmentation methods exploit encoder-decoder architectures to produce the desired pixel-wise segmentation prediction. The last layer of the decoders is typically a bilinear upsampling procedure to recover the final pixel-wise prediction. We empirically show that this oversimple and data-independent bilinear upsampling may lead to sub-optimal results.
In this work, we propose a data-dependent upsampling (DUpsampling) to replace bilinear, which takes advantages of the redundancy in the label space of semantic segmentation and is able to recover the pixel-wise prediction from low-resolution outputs of CNNs. The main advantage of the new upsampling layer lies in that with a relatively lower-resolution feature map such as 1/16 or 1/32 of the input size, we can achieve even better segmentation accuracy, significantly reducing computation complexity. This is made possible by 1) the new upsampling layer&#x27;s much improved reconstruction capability; and more importantly 2) the DUpsampling based decoder&#x27;s flexibility in leveraging almost arbitrary combinations of the CNN encoders&#x27; features. Experiments demonstrate that our proposed decoder outperforms the state-of-the-art decoder, with only 20% of computation. Finally, without any post-processing, the framework equipped with our proposed decoder achieves new state-of-the-art performance on two datasets: 88.1% mIOU on PASCAL VOC with 30% computation of the previously best model; and 52.5% mIOU on PASCAL Context.     </p></blockquote><p>如果有时间的话请阅读<a href="https://arxiv.org/pdf/1903.02120.pdf" target="_blank" rel="noopener noreferrer">原作</a>。本文只是对原作阅读的粗浅笔记。</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/survey">survey</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/decoder">decoder</a></li></ul></div><div class="col text--right col--3"><a aria-label="阅读 Decoders Matter for Semantic Segmentation - Data-Dependent Decoding Enables Flexible Feature Aggregation 的全文" href="/blog/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation"><b>阅读更多</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 16 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/PommesPeter.png" alt="PommesPeter"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://blog.pommespeter.com/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">PommesPeter</span></a></div><small class="avatar__subtitle" itemprop="description">I want to be strong. But it seems so hard.</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>这是一篇讲低光照人脸检测的论文。<a href="https://arxiv.org/pdf/2104.01984.pdf" target="_blank" rel="noopener noreferrer">原论文（HLA-Face Joint High-Low Adaptation for Low Light Face Detection）</a>。</p><ul><li>充分利用现有的正常光数据，并探索如何将面部探测器从正常光线调整到低光。这项任务的挑战是，正常和低光之间的差距对于像素级和物体级别来说太大而复杂。因此，大多数现有的lowlighenhance和适应方法不达到所需的performance。</li><li>本文是DARK FACE为基准，针对现有的正常照度图像，将图像调整成低照度图像，不需要标签</li><li>一个是像素级外观的差距，例如不足，照明，相机噪声和颜色偏置。另一个是正常和低光场景之间的物体级语义差异，包括但不限于路灯的存在，车辆前灯和广告板。传统的低光增强方法<!-- -->[5,6]<!-- -->设计用于提高视觉质量，因此不能填充语义差距，</li><li>通过使低光图像亮起并扭曲正常光图像，我们构建位于正常和低光之间的中间状态。</li></ul><p>摘要:</p><blockquote><p>Face detection in low light scenarios is challenging but vital to many practical applications, e.g., surveillance video, autonomous driving at night. Most existing face detectors heavily rely on extensive annotations, while col- lecting data is time-consuming and laborious. To reduce the burden of building new datasets for low light condi- tions, we make full use of existing normal light data and explore how to adapt face detectors from normal light to low light. The challenge of this task is that the gap between normal and low light is too huge and complex for both pixel-level and object-level. Therefore, most existing low- light enhancement and adaptation methods do not achieve desirable performance. To address the issue, we propose a joint High-Low Adaptation (HLA) framework. Through a bidirectional low-level adaptation and multi-task high- level adaptation scheme, our HLA-Face outperforms state- of-the-art methods even without using dark face labels for training. Our project is publicly available at: <!-- -->[https: //daooshee.github.io/HLA-Face-Website/]<!-- -->(https: //daooshee.github.io/HLA-Face-Website/)</p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/detection">detection</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/low-light">low-light</a></li></ul></div><div class="col text--right col--3"><a aria-label="阅读 HLA-Face Joint High-Low Adaptation for Low Light Face Detection 的全文" href="/blog/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection"><b>阅读更多</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/[06]DeepLab-Series">DeepLab Series</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 10 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | 键圈躺尸砖家</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>DeepLab系列中包含了三篇论文：DeepLab-v1、DeepLab-v2、DeepLab-v3。</p><p>DeepLab-v1：<a href="https://arxiv.org/abs/1412.7062" target="_blank" rel="noopener noreferrer">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a></p><p>DeepLab-v2：<a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="noopener noreferrer">Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></p><p>DeepLab-v3：<a href="https://arxiv.org/pdf/1706.05587.pdf" target="_blank" rel="noopener noreferrer">Rethinking Atrous Convolution for Semantic Image Segmentation</a></p><p>在这里我们将这三篇放在一起阅读。</p><p>后来甚至还出现了后续：</p><p>DeepLab-v3+：<a href="https://arxiv.org/abs/1802.02611" target="_blank" rel="noopener noreferrer">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a></p><p>不过暂时没有写进来的打算。</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/decoder">decoder</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/atrous-convolution">atrous-convolution</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/backbone">backbone</a></li></ul></div><div class="col text--right col--3"><a aria-label="阅读 DeepLab Series 的全文" href="/blog/[06]DeepLab-Series"><b>阅读更多</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 6 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/zeroRains.png" alt="Zerorains"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Zerorains</span></a></div><small class="avatar__subtitle" itemprop="description">life is but a span, I use python</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>论文名称：<a href="https://arxiv.org/abs/2103.11351" target="_blank" rel="noopener noreferrer">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></p><p>作者：<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+L" target="_blank" rel="noopener noreferrer">Li Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+D" target="_blank" rel="noopener noreferrer">Dong Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu%2C+Y" target="_blank" rel="noopener noreferrer">Yousong Zhu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tian%2C+L" target="_blank" rel="noopener noreferrer">Lu Tian</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shan%2C+Y" target="_blank" rel="noopener noreferrer">Yi Shan</a></p><p>期刊：CVPR2021</p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="主要结构">主要结构<a href="#主要结构" class="hash-link" aria-label="主要结构的直接链接" title="主要结构的直接链接">​</a></h2><p>DAB：Dataset-Aware Block(数据集感知块)</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">    作为网络的基本计算单元，有助于捕获o不同功能数据集之间的同质表示和异构统计。</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    主要由，一个数据集不变的卷积层，多个数据集特定的BatchNormal和一个激活层构成。</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>DAT：Dataset Alternation Training(数据集交替训练机制)</p><p>分割结果：</p><p><img loading="lazy" alt="image-20210505160138997" src="/assets/images/20210505160141image-20210505160138997-0005e31a81f1a3491314c4432158798b.png" width="923" height="770" class="img_ev3q"></p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/segmentation">segmentation</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/refinement">refinement</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cross-dataset-learning">cross-dataset-learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="阅读 Cross-Dataset Collaborative Learning for Semantic Segmentation 的全文" href="/blog/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation"><b>阅读更多</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks - A Survey</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 33 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | 键圈躺尸砖家</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>这篇论文是一篇对动态神经网络的综述，原论文<a href="http://arxiv.org/abs/2102.04906" target="_blank" rel="noopener noreferrer">&quot;Dynamic Neural Networks: A Survey&quot;</a>主要讲了：</p><ul><li>概念（Introduction）</li><li>常见的动态神经网络<ul><li>Instance-wise Dynamic Networks</li><li>Spatial-wise Dynamic Networks</li><li>Temporal-wise Dynamic Network</li></ul></li><li>推理和训练（Inference and Training）</li><li>常见应用和代表性工作（Applications）</li></ul><p>这篇论文对近些年吸引了很多研究者的动态神经网络进行了较为系统的总结概括。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="abstract摘要">Abstract（摘要）<a href="#abstract摘要" class="hash-link" aria-label="Abstract（摘要）的直接链接" title="Abstract（摘要）的直接链接">​</a></h2><blockquote><p>Dynamic neural network is an emerging research topic in deep learning. Compared to static models which have fixed computational graphs and parameters at the inference stage, dynamic networks can adapt their structures or parameters to different inputs, leading to notable advantages in terms of accuracy, computational efficiency, adaptiveness, etc. In this survey, we comprehensively review this rapidly developing area by dividing dynamic networks into three main categories: 1) instance-wise dynamic models that process each instance with data-dependent architectures or parameters; 2) spatial-wise dynamic networks that conduct adaptive computation with respect to different spatial locations of image data and 3) temporal-wise dynamic models that perform adaptive inference along the temporal dimension for sequential data such as videos and texts. The important research problems of dynamic networks, e.g., architecture design, decision making scheme, optimization technique and applications, are reviewed systematically. Finally, we discuss the open problems in this field together with interesting future research directions.</p></blockquote><p>动态神经网络近些年的相关研究逐渐变多，比起固定计算图的传统的静态神经网络，动态神经网络能够可以根据输入的具体数据调整它们的结构或是参数，同时在速度和精度方面占有优势。一种比喻是：“在输入较为简单时，动态神经网络可以很快；在输入较为复杂时，动态神经网络可以精度很高”。</p><p>这篇论文概括地介绍了动态神经网络是如何“动态”的，以及动态带来了怎样的优势。 </p><p>我看了这篇综述受益匪浅，如果有时间的话请阅读<a href="http://arxiv.org/abs/2102.04906" target="_blank" rel="noopener noreferrer">原作</a>。本文只是对原作阅读的粗浅笔记。</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/survey">survey</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/dynamic-neural-network">dynamic-neural-network</a></li></ul></div><div class="col text--right col--3"><a aria-label="阅读 Dynamic Neural Networks - A Survey 的全文" href="/blog/[08]Dynamic-Neural-Networks-A-Survey"><b>阅读更多</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-31T09:31:53.000Z" itemprop="datePublished">2023年12月31日</time> · <!-- -->阅读需 11 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.yuuza.net/visualDust.png" alt="Gavin Gong"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://gong.host" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gavin Gong</span></a></div><small class="avatar__subtitle" itemprop="description">Rubbish CVer | Poor LaTex speaker | Half stack developer | 键圈躺尸砖家</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h3 class="anchor anchorWithStickyNavbar_LWe7" id="这篇笔记的写作者是visualdust">这篇笔记的写作者是<a href="https://github.com/visualDust" target="_blank" rel="noopener noreferrer">VisualDust</a>。<a href="#这篇笔记的写作者是visualdust" class="hash-link" aria-label="这篇笔记的写作者是visualdust的直接链接" title="这篇笔记的写作者是visualdust的直接链接">​</a></h3><p>原论文<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener noreferrer">Feature Pyramid Networks for Object Detection</a>。</p><p>这篇论文就是大家熟知的FPN了。FPN是<strong>比较早期的一份工作</strong>（请注意，这篇论文只是多尺度特征融合的一种方式。不过这篇论文提出的比较早（CVPR2017），在当时看来是非常先进的），在当时具有很多亮点：FPN主要解决的是物体检测中的多尺度问题，通过简单的网络连接改变，在基本不增加原有模型计算量情况下，大幅度提升了小物体检测的性能。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="abstract摘要">Abstract（摘要）<a href="#abstract摘要" class="hash-link" aria-label="Abstract（摘要）的直接链接" title="Abstract（摘要）的直接链接">​</a></h2><blockquote><p>Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.</p></blockquote><p>这篇论文对以后的许多网络设计产生了较大的影响，推荐你阅读<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener noreferrer">原文</a>。这里只是对这篇论文的粗浅阅读笔记。</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/detection">detection</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/fpn">FPN</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/backbone">backbone</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/multi-scale-learning">multi-scale-learning</a></li></ul></div><div class="col text--right col--3"><a aria-label="阅读 Feature Pyramid Networks for Object Detection 的全文" href="/blog/[09]Feature-Pyramid-Networks-for-Object-Detection"><b>阅读更多</b></a></div></footer></article><nav class="pagination-nav" aria-label="博文列表分页导航"><a class="pagination-nav__link pagination-nav__link--next" href="/blog/page/2"><div class="pagination-nav__label">较旧的博文</div></a></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">鲁ICP备2021025239号-2 Copyright © 2023 neet-cv.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.416d7f00.js"></script>
<script src="/assets/js/main.d42e6425.js"></script>
</body>
</html>